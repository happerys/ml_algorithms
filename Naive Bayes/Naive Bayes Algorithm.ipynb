{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Naive Bayes Algorithm\n",
    "============\n",
    "\n",
    "> 朴素贝叶斯算法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对于给定的输入$X$，利用贝叶斯定理求出后验概率最大的输出$y$.\n",
    "\n",
    "> 设输入空间 $\\cal X \\subseteq R^n$ 为$n$维向量的集合，输出空间为类标记集合 $\\cal Y=\\{c_1, c_2, \\cdots , c_k\\}$ ，输入为特征向量 $x \\in \\cal X$，输出为类标记(class label)$y \\in \\cal Y$. $X$是定义在输入空间$\\cal X$上的随机向量，$Y$是定义在输出空间$\\cal Y$上的随机变量. $P(X,Y)$是$X$和$Y$的联合概率分布. 训练数据集$$T=\\{(x_1, y_1),(x_2, y_2),\\cdots,(x_k, y_k)\\}$$由$P(X,Y)$独立同分布产生.\n",
    "\n",
    "> 朴素贝叶斯公式推导：\n",
    "  ---------------\n",
    ">> 朴素贝叶斯分类时，对给定的输入 $x$，通过学习到的模型计算后验概率分布 $P(Y=c_k | X=x)$，** 将后验概率最大的类作为 $x$ 的类输出 **。\n",
    ">> 后验概率计算根据贝叶斯定理如下：\n",
    "$$P(Y=c_k|X=x) = \\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_{k} P(X=x|Y=c_k)P(Y=c_k)} \\qquad    (1)$$\n",
    ">> 朴素贝叶斯法对条件概率分布作了条件独立性假设.\n",
    "$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k)=\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k) \\qquad   (2)$$\n",
    ">> 把公式(2)代入公式(1)得到朴素贝叶斯分类的基本公式(3)：\n",
    "$$P(Y=c_k|X=x) = \\frac{P(Y=c_k)\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_{k}P(Y=c_k)\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)},\\;k=1,2,3,\\cdots,K \\qquad    (3)$$\n",
    ">> 朴素贝叶斯分类器表示如下：\n",
    "$$y=f(x)=arg\\;{max}_{c_k}\\frac{P(Y=c_k)\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_{k}P(Y=c_k)\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)}\\qquad   (4)$$\n",
    ">> 由于(4)中分母对所有的$c_k$都是相同的，所以，\n",
    "$$y=f(x)=arg\\;{max}_{c_k} P(Y=c_k)\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)\\qquad   (5)$$\n",
    "\n",
    "> 朴素贝叶斯算法:\n",
    "  ------------\n",
    ">> 输入：训练数据$T=\\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)\\}$，其中$x_i={(x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(n)})}^T$，$x_i^{(j)}$是第$i$个样本的第$j$个特征，$x_i^{(j)}\\in\\{{a_{j1},a_{j2},\\cdots,a_{jS_j}}\\}$，$a_{jl}$是第$j$个特征可能取得第l个值，$j=1,2,\\cdots,n,\\;l=1,2,\\cdots,S_j,\\;y_i\\in\\{c_1,c_2,\\cdots,c_K\\}$；实例$x$；\n",
    ">> 输出：实例$x$的分类.\n",
    "\n",
    ">> (1)计算先验概率及条件概率\n",
    "$$P(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)}{N},\\;k=1,2,\\cdots,K \\qquad   (6)$$\n",
    "$$P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^NI(y_i=c_k)} \\qquad   (7)$$\n",
    "$$j=1,2,\\cdots,n;\\;l=1,2,\\cdots,S_j;\\;k=1,2,\\cdots,K$$\n",
    "\n",
    ">> (2)对于给定的实例$x={(x^{(1)},x^{(2)},\\cdots,x^{(n)})}^T$，计算\n",
    "$$P(Y=c_k)\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)\\;k=1,2,\\cdots,K \\qquad   (8)$$\n",
    "\n",
    ">> (3)确定实例$x$的类\n",
    "$$y=arg\\;{max}_{c_k} P(Y=c_k)\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k) \\qquad   (9)$$\n",
    "\n",
    "> 拉普拉斯平滑\n",
    "  ----------\n",
    ">> 用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用**贝叶斯估计**。条件概率的贝叶斯估计是\n",
    "$$P_\\lambda(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\\lambda}{\\sum_{i=1}^NI(y_i=c_k)+S_j\\lambda}\\qquad(10)$$\n",
    ">> 式中$\\lambda\\geq0$.等价于在随机变量各个取值的频数上赋予一个正数$\\lambda>0$.当$\\lambda=0$时就是极大似然估计.取$\\lambda=1$，这时称为拉普拉斯平滑(Laplace smoothing).显然，对任何$l=1,2,\\cdots,K$，有\n",
    "$$P_\\lambda(X^{(j)}=a_{jl}|Y=c_k)>0$$\n",
    "$$\\sum_{k}^{S_j} P(X^{(j)}=a_{jl}|Y=c_k)=1$$\n",
    ">> 先验概率的贝叶斯估计是\n",
    "$$P_\\lambda(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)+\\lambda}{N+K\\lambda}$$\n",
    "\n",
    "> ***备注：参考资料《统计学习方法》，李航 著***\n",
    "\n",
    "> 贝叶斯算法的优缺点：\n",
    "  ---------------\n",
    ">> 优点：在数据较少的情况下仍然有效，可以处理多类别问题；\n",
    "\n",
    ">> 缺点：对于输入数据的准备方式较为敏感；\n",
    "\n",
    ">> 适用数据类型：标称型数据.\n",
    "\n",
    "Naive Bayes 实现\n",
    "========\n",
    "准备数据集\n",
    "--------\n",
    "> 由于Naive Bayes擅长NLP领域，故使用垃圾邮件数据集来训练及验证Naive Bayes的学习及验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取语料\n",
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "# 创建一个包含所有文档中出现的不重复词的列表\n",
    "def createVocabList(dataSet ): \n",
    "    vocabSet = set ([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) \n",
    "    # print vocabSet\n",
    "    return list(vocabSet)\n",
    "\n",
    "# （词袋模型）输入词汇表及某个文档，输出文档向量，向量的每个元素为1或0，分别表示词汇中的单词再输入文档中是否出现\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList) \n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "        else: \n",
    "            print \"the word：%s is not in my Vocabulary!\" % word \n",
    "    return returnVec\n",
    "\n",
    "# 将垃圾邮件以及正常邮件进行分词、构造词袋模型、分割训练以及测试集\n",
    "def spamTest():\n",
    "    import random\n",
    "    import numpy as np\n",
    "    docList = []; classList = []; fullText = []\n",
    "    for i in range(1, 26):\n",
    "        wordList = textParse(open(\"/Users/zhangxingbin/ml_algorithms/data/email/spam/%d.txt\" % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('/Users/zhangxingbin/ml_algorithms/data/email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = range(50); testSet=[]\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat = []; trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "\n",
    "    return np.asarray(trainMat), np.asarray(trainClasses), np.asarray(testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法简单实现\n",
    "---------\n",
    ">Naive Bayes算法实现主要思路如下：\n",
    "\n",
    "> * 根据公式(6)、(7)利用方法 occurrences 求解出先验概率及条件概率，其中使用的trick是将概率值转换到对数空间；\n",
    "\n",
    "> * 条件概率的求解过程使用了字典数据结构，该字典以标签为key，字典value值为该类别下各特征出现的次数，然后根据该字典计算概率值；\n",
    "\n",
    "> * 最后一个trick是把公式(9)等号两边取对数，一是对应步骤1、2中概率值在对数空间上的映射，二是将公式(9)的乘法运算转换为加法运算."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "\n",
    "# 计算对数概率值，概率值以字典的结构返回\n",
    "def occurrences(lists):\n",
    "    no_of_examples = len(lists)\n",
    "    prob = dict(Counter(lists))\n",
    "    for key in prob.keys():\n",
    "        prob[key] = math.log(prob[key] / float(no_of_examples))\n",
    "    return prob\n",
    "\n",
    "# 训练模型\n",
    "def naive_bayes(trainSet, labels, testSet):\n",
    "    classes = np.unique(labels)\n",
    "    rows, cols = np.shape(trainSet)\n",
    "    likelihoods = {}\n",
    "    # 以 label 为 key 初始化 likelihoods\n",
    "    for cls in classes:\n",
    "        likelihoods[cls] = defaultdict(list)\n",
    "\n",
    "    # 计算先验概率\n",
    "    class_probabilities = occurrences(labels)\n",
    "\n",
    "    # 根据 label 以及 features 统计 sample 中 feature出现的次数 --> 为计算条件概率做准备\n",
    "    for cls in classes:\n",
    "        # 抽取每个 label 对应的样本\n",
    "        row_indices = np.where(labels == cls)[0]\n",
    "        subset = trainSet[row_indices, :]\n",
    "        r, c = np.shape(subset)\n",
    "        for j in range(0, c):\n",
    "            likelihoods[cls][j] += list(subset[:, j])\n",
    "\n",
    "    # 计算条件概率\n",
    "    for cls in classes:\n",
    "        for j in range(0, cols):\n",
    "            likelihoods[cls][j] = occurrences(likelihoods[cls][j])\n",
    "            \n",
    "    # 测试集做测试\n",
    "    results = {}\n",
    "    for cls in classes:\n",
    "        class_probability = class_probabilities[cls]\n",
    "        for i in range(0, len(testSet)):\n",
    "            relative_values = likelihoods[cls][i]\n",
    "            if testSet[i] in relative_values.keys():\n",
    "                class_probability += relative_values[testSet[i]]\n",
    "            else:\n",
    "                class_probability += 0\n",
    "            results[cls] = class_probability\n",
    "    \n",
    "    result = -100\n",
    "    clazz = ''\n",
    "    for cls in classes:\n",
    "        if result < results[cls]:\n",
    "            result = results[cls]\n",
    "            clazz = cls\n",
    "\n",
    "    print \"testSet's label is: \", clazz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证Naive Bayes的实现\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testSet's label is:  1\n"
     ]
    }
   ],
   "source": [
    "# 获取训练集 trainMat，标签 trainClasses，测试集 testSet\n",
    "trainMat, trainClasses, testSet = spamTest()\n",
    "\n",
    "# 训练模型\n",
    "naive_bayes(trainMat, trainClasses, testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多项式、高斯、伯努利Naive Bayes的实现\n",
    "==================\n",
    "上述算法虽然使用了log的trick，但是没有考虑公式(10)的拉普拉斯平滑，下面给出多项式、高斯、伯努利带有拉普拉斯平滑的实现\n",
    "\n",
    "多项式 Naive Bayes (词频型)\n",
    "----------------\n",
    "> 多项式模型是以\"单词\"为统计单位，即统计某个单词在文档中出现的次数，当某个特征词在某个文档中出现多次，多项式模型会计算多次。这个模型符合$NLP$的做法。其基本原理如下：\n",
    ">> 在多项式模型中，设某文档$d=(t_1,t_2,\\cdots,t_k)$，$t_k$是该文档中出现过的单词，$\\color{#F00}{允许重复}$，则先验概率$$P(c)=\\frac{类c下单词总数}{整个训练样本的单词总数}$$\n",
    "\n",
    ">> 条件概率\n",
    "$$P(t_k|c)=\\frac{类c下单词t_k在各个文档中出现过的次数之和+1}{类c下单词总数+|V|}$$\n",
    "\n",
    ">> $V$是训练样本的单词表（即抽取单词，单词出现多次，只算一个），$|V|$则表示训练样本包含多少种单词。$P(t_k|c)$可以看作是单词$t_k$在证明$d$属于类$c$上提供了多大的证据，而$P(c)$则可以认为是类别$c$在整体上占多大比例(有多大可能性)。\n",
    "\n",
    "多项式 Naive Bayes实现\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultinomialNB(object):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        count_sample = X.shape[0]\n",
    "        separated = [[x for x, t in zip(X, y) if t == c] for c in np.unique(y)]\n",
    "        self.class_log_prior_ = [(np.log(len(i)) - np.log(count_sample)) for i in separated]\n",
    "        count = np.array([np.array(i).sum(axis=0) for i in separated]) + self.alpha\n",
    "        self.feature_log_prob_ = np.log(count)-np.log(count.sum(axis=1)[np.newaxis].T)\n",
    "        return self\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        return [(self.feature_log_prob_ * x).sum(axis=1) + self.class_log_prior_ for x in X]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [2, 1, 0, 0, 0, 0],\n",
    "    [2, 0, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 0, 0],\n",
    "    [1, 0, 0, 0, 1, 1]\n",
    "])\n",
    "\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "nb = MultinomialNB().fit(X, y)\n",
    "X_test = np.array([[2, 1, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1]])\n",
    "print(nb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "伯努利 Naive Bayes (文档型)\n",
    "------------------------\n",
    "> 伯努利模型是以“文档”为统计单位，即统计某个特征词出现在多少个文档当中，若某个特征词在某个文档中出现了多次，那么伯努利模型只是计算一次。\n",
    "\n",
    ">> $P(c)= \\frac{类c下文件总数}{整个训练样本的文件总数}$\n",
    "\n",
    ">> $P(t_k|c)=\\frac{类c下包含单词t_k的文件数+1}{类c下文件总数+2}$\n",
    "\n",
    ">> 备注：$P(t_k|c)$中的(类$c$下文件总数+2) 不是网上流传的(类$c$下单词总数+2)。\n",
    "\n",
    "伯努利 Naive Bayes实现\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNB(object):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        count_sample = X.shape[0]\n",
    "        separated = [[x for x, t in zip(X, y) if t == c] for c in np.unique(y)]\n",
    "        self.class_log_prior_ = [(np.log(len(i)) - np.log(count_sample)) for i in separated]\n",
    "        count = np.array([np.array(i).sum(axis=0) for i in separated]) + self.alpha\n",
    "        smoothing = 2 * self.alpha\n",
    "        n_doc = np.array([len(i) + smoothing for i in separated])\n",
    "        self.feature_prob_ = count / n_doc[np.newaxis].T\n",
    "        return self\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        return [(np.log(self.feature_prob_) * x +\n",
    "                 np.log(1 - self.feature_prob_) * np.abs(x - 1)\n",
    "                 ).sum(axis=1) + self.class_log_prior_ for x in X]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array([[1, 0, 0, 0, 1, 1], [1, 0, 0, 0, 1, 1]])\n",
    "\n",
    "nb = BernoulliNB(alpha=1).fit(np.where(X > 0, 1, 0), y)\n",
    "print(nb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高斯 Naive Bayes\n",
    "--------------\n",
    "> 当一些特征是连续性的值时，就可以采用高斯模型，一般是假设特征的分布是高斯分布。\n",
    ">> $$P(x_i|y_k)=\\frac{1}{\\sqrt{2\\pi\\sigma^2y_k}}exp(-\\frac{(x_i-\\mu_{yk})^2}{2\\sigma^2y_k})$$\n",
    "\n",
    "高斯 Naive Bayes 实现\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        separated = [[x for x, t in zip(X, y) if t == c] for c in np.unique(y)]\n",
    "        self.model = np.array([np.c_[np.mean(i, axis=0), np.std(i, axis=0)] for i in separated])\n",
    "        return self\n",
    "\n",
    "    def _prob(self, x, mean, std):\n",
    "        exponent = np.exp(- ((x - mean)**2 / (2 * std**2)))\n",
    "        return np.log(exponent) - np.log((np.sqrt(2 * np.pi) * std))\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        return [[sum(self._prob(i, *s) for s, i in zip(summaries, x)) for summaries in self.model] for x in X]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return sum(self.predict(X) == y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
