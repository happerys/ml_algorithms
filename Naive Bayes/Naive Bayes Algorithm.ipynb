{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Naive Bayes Algorithm\n",
    "============\n",
    "\n",
    "> 朴素贝叶斯算法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对于给定的输入$X$，利用贝叶斯定理求出后验概率最大的输出$y$.\n",
    "\n",
    "> 设输入空间 $\\cal X \\subseteq R^n$ 为$n$维向量的集合，输出空间为类标记集合 $\\cal Y=\\{c_1, c_2, \\cdots , c_k\\}$ ，输入为特征向量 $x \\in \\cal X$，输出为类标记(class label)$y \\in \\cal Y$. $X$是定义在输入空间$\\cal X$上的随机向量，$Y$是定义在输出空间$\\cal Y$上的随机变量. $P(X,Y)$是$X$和$Y$的联合概率分布. 训练数据集$$T=\\{(x_1, y_1),(x_2, y_2),\\cdots,(x_k, y_k)\\}$$由$P(X,Y)$独立同分布产生.\n",
    "\n",
    "> 朴素贝叶斯公式推导：\n",
    "  ---------------\n",
    ">> 朴素贝叶斯分类时，对给定的输入 $x$，通过学习到的模型计算后验概率分布 $P(Y=c_k | X=x)$，** 将后验概率最大的类作为 $x$ 的类输出 **。\n",
    ">> 后验概率计算根据贝叶斯定理如下：\n",
    "$$P(Y=c_k|X=x) = \\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_{k} P(X=x|Y=c_k)P(Y=c_k)} \\qquad    (1)$$\n",
    ">> 朴素贝叶斯法对条件概率分布作了条件独立性假设.\n",
    "$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k)=\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k) \\qquad   (2)$$\n",
    ">> 把公式(2)代入公式(1)得到朴素贝叶斯分类的基本公式(3)：\n",
    "$$P(Y=c_k|X=x) = \\frac{P(Y=c_k)\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_{k}P(Y=c_k)\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)},\\;k=1,2,3,\\cdots,K \\qquad    (3)$$\n",
    ">> 朴素贝叶斯分类器表示如下：\n",
    "$$y=f(x)=arg\\;{max}_{c_k}\\frac{P(Y=c_k)\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_{k}P(Y=c_k)\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)}\\qquad   (4)$$\n",
    ">> 由于(4)中分母对所有的$c_k$都是相同的，所以，\n",
    "$$y=f(x)=arg\\;{max}_{c_k} P(Y=c_k)\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)\\qquad   (5)$$\n",
    "\n",
    "> 朴素贝叶斯算法:\n",
    "  ------------\n",
    ">> 输入：训练数据$T=\\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)\\}$，其中$x_i={(x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(n)})}^T$，$x_i^{(j)}$是第$i$个样本的第$j$个特征，$x_i^{(j)}\\in\\{{a_{j1},a_{j2},\\cdots,a_{jS_j}}\\}$，$a_{jl}$是第$j$个特征可能取得第l个值，$j=1,2,\\cdots,n,\\;l=1,2,\\cdots,S_j,\\;y_i\\in\\{c_1,c_2,\\cdots,c_K\\}$；实例$x$；\n",
    ">> 输出：实例$x$的分类.\n",
    "\n",
    ">> (1)计算先验概率及条件概率\n",
    "$$P(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)}{N},\\;k=1,2,\\cdots,K \\qquad   (6)$$\n",
    "$$P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^NI(y_i=c_k)} \\qquad   (7)$$\n",
    "$$j=1,2,\\cdots,n;\\;l=1,2,\\cdots,S_j;\\;k=1,2,\\cdots,K$$\n",
    "\n",
    ">> (2)对于给定的实例$x={(x^{(1)},x^{(2)},\\cdots,x^{(n)})}^T$，计算\n",
    "$$P(Y=c_k)\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)\\;k=1,2,\\cdots,K \\qquad   (8)$$\n",
    "\n",
    ">> (3)确定实例$x$的类\n",
    "$$y=arg\\;{max}_{c_k} P(Y=c_k)\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k) \\qquad   (9)$$\n",
    "\n",
    ">> 公式(9)在对数空间上表示为如下的线性模型：\n",
    "$$\\log p(C_k) + \\sum_{i=1}^n x_i \\cdot \\log p_{ki}=b+\\bf w_{\\kappa}^T \\bf x$$\n",
    "\n",
    "> 拉普拉斯平滑\n",
    "  ----------\n",
    ">> 用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用**贝叶斯估计**。条件概率的贝叶斯估计是\n",
    "$$P_\\lambda(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\\lambda}{\\sum_{i=1}^NI(y_i=c_k)+S_j\\lambda}\\qquad(10)$$\n",
    ">> 式中$\\lambda\\geq0$.等价于在随机变量各个取值的频数上赋予一个正数$\\lambda>0$.当$\\lambda=0$时就是极大似然估计.取$\\lambda=1$，这时称为拉普拉斯平滑(Laplace smoothing).显然，对任何$l=1,2,\\cdots,K$，有\n",
    "$$P_\\lambda(X^{(j)}=a_{jl}|Y=c_k)>0$$\n",
    "$$\\sum_{k}^{S_j} P(X^{(j)}=a_{jl}|Y=c_k)=1$$\n",
    ">> 先验概率的贝叶斯估计是\n",
    "$$P_\\lambda(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)+\\lambda}{N+K\\lambda}$$\n",
    "\n",
    "> ***备注：参考资料《统计学习方法》，李航 著***\n",
    "\n",
    "> 贝叶斯算法的优缺点：\n",
    "  ---------------\n",
    ">> 优点：在数据较少的情况下仍然有效，可以处理多类别问题；\n",
    "\n",
    ">> 缺点：对于输入数据的准备方式较为敏感；\n",
    "\n",
    ">> 适用数据类型：标称型数据.\n",
    "\n",
    "Naive Bayes 实现\n",
    "========\n",
    "准备数据集\n",
    "--------\n",
    "> 由于Naive Bayes擅长NLP领域，故使用垃圾邮件数据集来训练及验证Naive Bayes的学习及验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取语料\n",
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split('\\W+', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "# 创建一个包含所有文档中出现的不重复词的列表\n",
    "def createVocabList(dataSet ): \n",
    "    vocabSet = set ([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) \n",
    "    # print vocabSet\n",
    "    return list(vocabSet)\n",
    "\n",
    "# （词袋模型）输入词汇表及某个文档，输出文档向量，向量的每个元素为1或0，分别表示词汇中的单词再输入文档中是否出现\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList) \n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "        else: \n",
    "            print(\"the word：%s is not in my Vocabulary!\" % word )\n",
    "    return returnVec\n",
    "\n",
    "# 将垃圾邮件以及正常邮件进行分词、构造词袋模型、分割训练以及测试集\n",
    "def spamTest():\n",
    "    import random\n",
    "    import numpy as np\n",
    "    docList = []; classList = []; fullText = []\n",
    "    for i in range(1, 26):\n",
    "        wordList = textParse(open(\"../data/email/spam/%d.txt\" % i, encoding = 'cp1252').read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('../data/email/ham/%d.txt' % i, encoding = 'cp1252').read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = list(range(0, 50)); testSet=[]\n",
    "    for i in list(range(0, 10)):\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat = []; trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "\n",
    "    return np.asarray(trainMat), np.asarray(trainClasses), np.asarray(testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法简单实现\n",
    "---------\n",
    ">Naive Bayes算法实现主要思路如下：\n",
    "\n",
    "> * 根据公式(6)、(7)利用方法 occurrences 求解出先验概率及条件概率，其中使用的trick是将概率值转换到对数空间；\n",
    "\n",
    "> * 条件概率的求解过程使用了字典数据结构，该字典以标签为key，字典value值为该类别下各特征出现的次数，然后根据该字典计算概率值；\n",
    "\n",
    "> * 最后一个trick是把公式(9)等号两边取对数，一是对应步骤1、2中概率值在对数空间上的映射，二是将公式(9)的乘法运算转换为加法运算."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "\n",
    "# 计算对数概率值，概率值以字典的结构返回\n",
    "def occurrences(lists):\n",
    "    no_of_examples = len(lists)\n",
    "    prob = dict(Counter(lists))\n",
    "    for key in prob.keys():\n",
    "        prob[key] = math.log(prob[key] / float(no_of_examples))\n",
    "    return prob\n",
    "\n",
    "# 训练模型\n",
    "def naive_bayes(trainSet, labels, testSet):\n",
    "    classes = np.unique(labels)\n",
    "    rows, cols = np.shape(trainSet)\n",
    "    likelihoods = {}\n",
    "    # 以 label 为 key 初始化 likelihoods\n",
    "    for cls in classes:\n",
    "        likelihoods[cls] = defaultdict(list)\n",
    "\n",
    "    # 计算先验概率\n",
    "    class_probabilities = occurrences(labels)\n",
    "\n",
    "    # 根据 label 以及 features 统计 sample 中 feature出现的次数 --> 为计算条件概率做准备\n",
    "    for cls in classes:\n",
    "        # 抽取每个 label 对应的样本\n",
    "        row_indices = np.where(labels == cls)[0]\n",
    "        subset = trainSet[row_indices, :]\n",
    "        r, c = np.shape(subset)\n",
    "        for j in range(0, c):\n",
    "            likelihoods[cls][j] += list(subset[:, j])\n",
    "\n",
    "    # 计算条件概率\n",
    "    for cls in classes:\n",
    "        for j in range(0, cols):\n",
    "            likelihoods[cls][j] = occurrences(likelihoods[cls][j])\n",
    "            \n",
    "    # 测试集做测试\n",
    "    results = {}\n",
    "    for cls in classes:\n",
    "        class_probability = class_probabilities[cls]\n",
    "        for i in range(0, len(testSet)):\n",
    "            relative_values = likelihoods[cls][i]\n",
    "            if testSet[i] in relative_values.keys():\n",
    "                class_probability += relative_values[testSet[i]]\n",
    "            else:\n",
    "                class_probability += 0\n",
    "            results[cls] = class_probability\n",
    "    \n",
    "    result = -100\n",
    "    clazz = ''\n",
    "    for cls in classes:\n",
    "        if result < results[cls]:\n",
    "            result = results[cls]\n",
    "            clazz = cls\n",
    "\n",
    "    print(\"testSet's label is: \", clazz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证Naive Bayes的实现\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testSet's label is:  1\n"
     ]
    }
   ],
   "source": [
    "# 获取训练集 trainMat，标签 trainClasses，测试集 testSet\n",
    "trainMat, trainClasses, testSet = spamTest()\n",
    "\n",
    "# 训练模型\n",
    "naive_bayes(trainMat, trainClasses, testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多项式、高斯、伯努利Naive Bayes的实现\n",
    "==================\n",
    "上述算法虽然使用了log的trick，但是没有考虑公式(10)的拉普拉斯平滑，下面给出多项式、高斯、伯努利带有拉普拉斯平滑的实现\n",
    "\n",
    "多项式 Naive Bayes (词频型)\n",
    "----------------\n",
    "> MultinomialNB 实现了数据服从多项式分布时的贝叶斯算法，它也是文本分类领域的两种典型算法之一(这里数据通常以词向量的形式表示，tf-idf向量在这里也表现的很好)。 这个分布被参数化成向量：$\\theta_y = (\\theta_{y1},\\cdots,\\theta_{yn})$\n",
    "\n",
    "> 对于每一个类别$y$，参数$n$表示特征数量（文本分类中表示词向量的大小）\n",
    "\n",
    "> $\\theta_{yi}$表示有$P(x_i|y)$的概率对于特征$i$在一个样本中的被类$y$所拥有。\n",
    "\n",
    "> 参数$\\theta_y$被平滑的极大似然估计法所估计，也就是说，相关频率计算：\n",
    "$$\\hat{\\theta}_{yi}=\\frac{N_{yi}+\\alpha}{N_y+\\alpha n}$$\n",
    "\n",
    "> 这里$N_{yi}=\\sum_{x \\in T}x_i$是特征$i$在训练集$T$中，在一个属于$y$类的样本中出现的次数，而$N_Y=\\sum_{i=1}^{\\mid T \\mid}N_{yi}$是在类$y$中所有的特征的数量和\n",
    "\n",
    "> 平滑先验$\\alpha \\geq 0$可引入不在训练样本中的特征，同时防止0概率在未来的计算中出现。如果$\\alpha = 1$，称为拉普拉斯平滑。\n",
    "#### NLP中的理解\n",
    "> 多项式模型是以\"单词\"为统计单位，即统计某个单词在文档中出现的次数，当某个特征词在某个文档中出现多次，多项式模型会计算多次。这个模型符合$NLP$的做法。其基本原理如下：\n",
    ">> 在多项式模型中，设某文档$d=(t_1,t_2,\\cdots,t_k)$，$t_k$是该文档中出现过的单词，$\\color{#F00}{允许重复}$，则先验概率$$P(c)=\\frac{类c下单词总数}{整个训练样本的单词总数}$$\n",
    "\n",
    ">> 条件概率\n",
    "$$P(t_k|c)=\\frac{类c下单词t_k在各个文档中出现过的次数之和+1}{类c下单词总数+|V|}$$\n",
    "\n",
    ">> $V$是训练样本的单词表（即抽取单词，单词出现多次，只算一个），$|V|$则表示训练样本包含多少种单词。$P(t_k|c)$可以看作是单词$t_k$在证明$d$属于类$c$上提供了多大的证据，而$P(c)$则可以认为是类别$c$在整体上占多大比例(有多大可能性)。\n",
    "\n",
    "多项式 Naive Bayes 实现\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB(object):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        # 初始化默认拉普拉斯平滑参数\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ''' 计算先验概率 '''\n",
    "        # 获取样本大小\n",
    "        count_sample = X.shape[0]\n",
    "        # 根据标签将样本分开\n",
    "        separated = [[x for x, t in zip(X, y) if t == c] for c in np.unique(y)]\n",
    "        self.class_log_prior_ = [(np.log(len(i)) - np.log(count_sample)) for i in separated]\n",
    "        \n",
    "        ''' 计算条件概率 '''\n",
    "        # 拉普拉斯平滑\n",
    "        count = np.array([np.array(i).sum(axis=0) for i in separated]) + self.alpha\n",
    "        # 极大似然估计\n",
    "        self.feature_log_prob_ = np.log(count) - np.log(count.sum(axis=1)[np.newaxis].T)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        ''' 对应公式(9): 先验概率 / 条件概率 => log(先验概率) + log(条件概率)'''\n",
    "        return [(self.feature_log_prob_ * x).sum(axis=1) + self.class_log_prior_ for x in X]\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' 返回最大概率值对应的类别 '''\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证多项式模型\n",
    "X, y, X_test = spamTest()\n",
    "\n",
    "nb = MultinomialNB().fit(X, y)\n",
    "X_test = np.array([[2, 1, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1]])\n",
    "nb.predict(X[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "伯努利 Naive Bayes (文档型)\n",
    "------------------------\n",
    "\n",
    "> BernoulliNB 实现了对于服从多元伯努利分布的数据的朴素贝叶斯训练和分类算法；对于大量特征，每一个特征都是一个0-1变量 (Bernoulli, boolean)。 因此，这个类要求样本集合以 0-1 特征向量的方式展现。如果接收到了其他类型的数据作为参数，一个 BernoulliNB 实例会把输入数据二元化(取决于 binarize 参数设置)\n",
    "\n",
    "> 朴素贝叶斯的伯努利模型是基于以下公式：\n",
    "$$P(x_i|y)=P(i|y)x_i + (1-P(i|y))(1-x_i) \\qquad (11)$$ \n",
    "\n",
    "> 其似然函数如下：\n",
    "$$P(x|C_k)=\\prod_{i=1}^n p_{k_i}^{x_i}{(1-p_{k_i})}^{(1-x_i)} $$\n",
    "\n",
    "> 其中$p_{k_i}$表示类别$C_k$生成term $w_i$的概率。这个模型通常用于短文本分类。\n",
    "\n",
    "> 在文本分类的情境中，被用来训练和使用这一分类器的是词语同现向量 (word occurrence vectors) 而不是词频向量 (word count vectors)。 BernoulliNB 可能尤其会在小数据集时表现良好。如果时间允许，推荐试用以上所有模型进行评价。\n",
    "\n",
    "> 伯努利模型是以“文档”为统计单位，即统计某个特征词出现在多少个文档当中，若某个特征词在某个文档中出现了多次，那么伯努利模型只是计算一次。\n",
    "\n",
    ">> 先验概率：$$P(c)= \\frac{类c下文件总数}{整个训练样本的文件总数}$$\n",
    "\n",
    ">> 条件概率：$$P(t_k|c)=\\frac{类c下包含单词t_k的文件数+1}{类c下文件总数+2}$$\n",
    "\n",
    "伯努利 Naive Bayes实现\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BernoulliNB(object):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        # 初始化默认拉普拉斯平滑参数\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ''' 计算先验概率 '''\n",
    "        count_sample = X.shape[0]\n",
    "        separated = [[x for x, t in zip(X, y) if t == c] for c in np.unique(y)]\n",
    "        self.class_log_prior_ = [(np.log(len(i)) - np.log(count_sample)) for i in separated]\n",
    "        \n",
    "        ''' 计算条件概率 '''\n",
    "        count = np.array([np.array(i).sum(axis=0) for i in separated]) + self.alpha\n",
    "        smoothing = 2 * self.alpha\n",
    "        n_doc = np.array([len(i) + smoothing for i in separated])\n",
    "        self.feature_prob_ = count / n_doc[np.newaxis].T\n",
    "        return self\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        ''' 公式(11)的实现 '''\n",
    "        return [(np.log(self.feature_prob_) * x + np.log(1 - self.feature_prob_) * np.abs(x - 1)).sum(axis=1) + self.class_log_prior_ for x in X]\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' 返回最大概率值对应的类别 '''\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# 验证伯努利朴素贝叶斯\n",
    "X = np.array([\n",
    "    [2,1,0,0,0,0],\n",
    "    [2,0,1,0,0,0],\n",
    "    [1,0,0,1,0,0],\n",
    "    [1,0,0,0,1,1]\n",
    "])\n",
    "\n",
    "y = np.array([0,0,0,1])\n",
    "\n",
    "X_test = np.array([[1, 0, 0, 0, 1, 1], [1, 0, 0, 0, 1, 1]])\n",
    "\n",
    "nb = BernoulliNB(alpha=1).fit(np.where(X > 0, 1, 0), y)\n",
    "print(nb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高斯 Naive Bayes\n",
    "--------------\n",
    "> 当一些特征是连续性的值时，就可以采用高斯模型，一般是假设特征的分布是高斯分布。假设训练集中有一个连续属性$x$。我们首先对数据根据类别分类，然后计算每个类别中$x$的均值和方差。令$\\mu_c$表示为$x$在$c$类上的均值，令$\\sigma^2_c$为$x$在$c$类上的方差。在给定类中某个值的概率 $P(x=v|c)$，可以通过将$v$表示为均值为$\\mu_c$，方差为$\\sigma^2_c$的正态分布计算出来。\n",
    "\n",
    ">> 条件概率\n",
    " $$P(x=v|c)=\\frac{1}{\\sqrt{2\\pi\\sigma^2y_c}} e^{-\\frac{(v-\\mu_c)^2}{2{\\sigma^2}_c}} \\qquad(12)$$\n",
    "\n",
    "> 处理连续数值问题的另一种常用的技术是通过离散化连续数值的方法。通常，当训练样本数量较少或者是精确的分布已知时，通过概率分布的方法是一种更好的选择。 在大量样本的情形下离散化的方法表现更优，因为大量的样本可以学习到数据的分布。由于朴素贝叶斯是一种典型的用到大量样本的方法（越大计算量的模型可以产生越高的分类精确度），所以朴素贝叶斯方法都用到离散化方法，而不是概率分布估计的方法。 \n",
    "\n",
    "高斯 Naive Bayes 实现\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GaussianNB(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ''' 将样本按照类别分开，计算出分开后样本的均值以及方差 '''\n",
    "        separated = [[x for x, t in zip(X, y) if t == c] for c in np.unique(y)]\n",
    "        self.model = np.array([np.c_[np.mean(i, axis=0), np.std(i, axis=0)] for i in separated])\n",
    "        return self\n",
    "\n",
    "    def _prob(self, x, mean, std):\n",
    "        ''' 公式(12)的实现 '''\n",
    "        exponent = np.exp(- ((x - mean)**2 / (2 * std**2)))\n",
    "        return np.log(exponent) - np.log((np.sqrt(2 * np.pi) * std))\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        return [[sum(self._prob(i, *s) for s, i in zip(summaries, x)) for summaries in self.model] for x in X]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return sum(self.predict(X) == y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.91893853320467267"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用鸢尾花数据验证高斯朴素贝叶斯\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "nb = GaussianNB().fit(iris.data, iris.target)\n",
    "\n",
    "nb._prob(0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
