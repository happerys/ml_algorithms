{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear Regression Algorithm\n",
    "基本形式\n",
    "------\n",
    "> 给定由$d$个属性描述的示例$x=(x_1;x_2;\\dots ;x_d)$，其中$x_i$是$x$的第$i$个属性上的取值，线性模型$(linear\\; model)$试图学得一个通过属性的线性组合来进行预测的函数，即\n",
    "$$f(x)=\\omega_1x_1+\\omega_2x_2+\\dots+\\omega_dx_d+b$$\n",
    "> 一般用向量的形式书写\n",
    "$$f(\\bf x)=\\omega^{\\bf T} \\bf x+b$$\n",
    "> 其中，$\\omega = (\\omega_1;\\omega_2;\\dots;\\omega_d)$\n",
    "\n",
    "线性回归\n",
    "------\n",
    "> 给定数据集$D=\\{(x_1,y_1),(x_2,y_2),\\dots,(x_m,y_m)\\}$，其中，$x_i=(x_{i1};x_{i2};\\dots;x_{id}),y_i\\subseteq R$。“线性回归”$（linear\\; regression）$试图学得一个线性模型以尽可能地预测实值输出标记。\n",
    "\n",
    "> 线性回归试图学得$f(x_i)=\\omega x_i + b$，使得$f(x_i)\\approx y_i$\n",
    "\n",
    "> **均方误差**是回归任务中最常用的性能度量，因此可以试图让均方误差最小化来求解$\\omega$和$b$\n",
    "$$\\begin{align}\n",
    "(\\omega^*,b^*) &={arg \\; min}_{(\\omega,b)}\\sum_{i=1}^m{(f(x_i)-y_i)}^2  \\\\\n",
    "&={arg \\; min}_{(\\omega,b)}\\sum_{i=1}^m{(y_i-\\omega x_i - b)}^2 \\qquad (1)\n",
    "\\end{align}$$\n",
    "\n",
    "> 均方误差有非常好的几何意义，它对应了重用的欧几里得距离或简称“欧氏距离”$(Eucliean\\;distanc)$. 基于均方误差最小化来进行模型求解的方法称为“最小二乘法”$（least\\;square\\;method）$. 在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。\n",
    "\n",
    "> 求解$\\omega$和$b$使$E_{(\\omega, b)}=\\sum_{i=1}^m(y_i-\\omega x_i -b)^2$最小化的过程，称为线性回归模型的最小二乘“参数估计”（$parameter\\;estimetion$）.我们可以将$E_{(\\omega,b)}$分别对$\\omega$和$b$求导，得到\n",
    "$$\\begin{align}\n",
    "& \\frac{\\partial E_{(\\omega, b)}}{\\partial \\omega }=2\\bigl(\\omega \\sum_{i=1}^m x_i^2-\\sum_{i=1}^m(y_i-b)x_i\\bigr)\\qquad (2) \\\\\n",
    "& \\frac{\\partial E_{(\\omega, b)}}{\\partial b}=2\\bigl(mb-\\sum_{i=1}^m(y_i-\\omega x_i)\\bigr)\\qquad (3)\n",
    "\\end{align}$$\n",
    "\n",
    "> 然后令(2)、(3)为零可得$\\omega$和$b$最优解的闭式（$closed-form$）解\n",
    "$$\\begin{align}\n",
    "& \\omega=\\frac{\\sum_{i=1}^my_i(x_i-\\bar{x})}{\\sum_{i=1}^mx_i^2-{\\frac{1}{m}}{\\bigl({\\sum_{i=1}^mx_i}\\bigr)}^2}\\qquad (4) \\\\\n",
    "& b={\\frac{1}{m}}\\sum_{i=1}^m(y_i-\\omega x_i)\\qquad(5)\n",
    "\\end{align}$$\n",
    "> 其中$\\bar{x}=\\frac{1}{m}\\sum_{i=1}^mx_i$为$x$的均值.\n",
    "\n",
    "多元线性回归$(multivariate\\;linear\\;regression)$\n",
    "----------------------------------------\n",
    "> 对于$\\;f(\\bf {x_i})={\\bf \\omega}^T \\bf {x_i} + b\\;$把$\\omega$和$b$吸收入向量形式$\\;\\hat{\\omega}=(\\omega;b)\\;$，相应的，把数据集$D$表示为一个$m\\times (d+1)$大小的矩阵$X$，其中每行对于于一个示例，该行前$d$个属性值，最后一个元素恒置为1，即\n",
    "$$X=\n",
    "    \\begin{pmatrix}\n",
    "        x_{11} & x_{12} & \\cdots & x_{1d} & 1 \\\\\n",
    "        x_{21} & x_{22} & \\cdots & x_{2d} & 1 \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "        x_{m1} & x_{m2} & \\cdots & x_{md} & 1 \\\\\n",
    "    \\end{pmatrix}\n",
    "    =\n",
    "    \\begin{pmatrix}\n",
    "       x_1^T & 1 \\\\\n",
    "       x_2^T & 1 \\\\\n",
    "       \\vdots & \\vdots \\\\\n",
    "       x_m^T & 1\\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "> 再把标记也写成向量的形式$y=(y_1;y_2;\\cdots;y_m)$，则根据(1)式有\n",
    "$$\\hat{\\omega}^*=arg\\;min_{\\hat{\\omega}}(y-X\\hat{\\omega})^T(y-X\\hat{\\omega})$$\n",
    "> 令$E_{\\hat{\\omega}}=\\frac{1}{2}(X\\hat{\\omega}-y)^2$，转换为矩阵运算如下\n",
    "$$\\begin{align}\n",
    "E_{\\hat{\\omega}} &=\\frac{1}{2}(X\\hat{\\omega}-y)^T(X\\hat{\\omega}-y) \\\\\n",
    "& =\\frac{1}{2}(\\hat{\\omega}^T X^T-y^T)(X\\hat{\\omega}-y) \\\\\n",
    "& =\\frac{1}{2}(\\hat{\\omega}^TX^TX\\hat{\\omega}-\\hat{\\omega}^TX^Ty-y^TX\\hat{\\omega}+y^Ty) \\\\\n",
    "& =\\frac{1}{2}(\\hat{\\omega}^TX^TX\\hat{\\omega}-y^TX\\hat{\\omega}-y^TX\\hat{\\omega}+y^Ty) \\\\\n",
    "& =\\frac{1}{2}(\\hat{\\omega}^TX^TX\\hat{\\omega}-2y^TX\\hat{\\omega}+y^Ty)\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "> 对$\\hat{\\omega}$求导得到\n",
    ">> 矩阵求导公式： \n",
    "$$\\begin{align}\n",
    "\\frac{dAB}{dB}&=A^T \\\\\n",
    "\\frac{dA^TB}{dA}&=B \\\\\n",
    "\\frac{dX^TAX}{dX}&=2AX \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "> \n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E_{\\hat{\\omega}}}{\\partial \\hat{\\omega}} &= \\frac{1}{2}(2X^TX\\omega-2X^Ty-0)    \\\\\n",
    "&=\\frac{1}{2}(2X^TX\\omega-2X^Ty)  \\\\\n",
    "&=X^TX\\omega-X^Ty  \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "> 当$X^TX$为满秩矩阵（$full-rank \\; matrix$）或正定矩阵（$positive\\;definite\\;matrix$）时，令 $\\frac{\\partial E_{\\hat{\\omega}}}{\\partial \\hat{\\omega}}=0$\n",
    "$$\\begin{align}\n",
    "X^TX\\hat{\\omega}-X^Ty&=0 \\\\\n",
    "X^TX\\hat{\\omega}&=X^Ty \\\\\n",
    "\\hat{\\omega}&=(X^TX)^{-1}X^Ty \\qquad (6)\\\\\n",
    "\\end{align}$$\n",
    "> 其中$(X^TX)^{-1}$是矩阵$(X^TX)$的逆矩阵。令$\\; \\hat{x}_i=(x_i, 1)\\;$故，可得多元线性回归模型为\n",
    "$$f(\\hat{x}_i)={\\hat{x}_i}^T(X^TX)^{-1}X^Ty$$\n",
    "\n",
    ">> **上述内容部分参考周志华老师的《机器学习》**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则化(regularization)\n",
    "------------------\n",
    "> 对于(6)式，如果**矩阵$X$有$d>m$，则$X^TX$不是满秩的**，或者**某些特征之间的线性相关性比较大时**，$\\hat{\\omega}$会有无穷多个解。\n",
    "如果从所有可行解里随机选一个的话，很可能并不是真正好的解，此时模型overfitting了。解决 overfitting 最常用的办法就是 regularization\n",
    "\n",
    "### 1、LASSO Regression(least absolute shrinkage and selection operator)\n",
    "> LASSO Regression表示如下：\n",
    "$$\\begin{align}\n",
    "E_{\\hat{\\omega}} &=\\frac{1}{2}(X\\hat{\\omega}-y)^T(X\\hat{\\omega}-y)+\\lambda \\begin{Vmatrix} \\omega \\end{Vmatrix}_1\\\\\n",
    "& =\\frac{1}{2}(\\hat{\\omega}^TX^TX\\hat{\\omega}-2y^TX\\hat{\\omega}+y^Ty)+\\lambda \\begin{Vmatrix} \\omega \\end{Vmatrix}_1\n",
    "\\end{align}$$\n",
    "\n",
    "> **1、gradient存在，此时$\\bar{\\omega}^j\\neq 0$**\n",
    "$$\\left. \\frac{\\partial E_{\\hat{\\omega}}}{\\partial \\hat{\\omega}} \\right| _{\\; \\omega^j}=0$$\n",
    "\n",
    "> 所以$$(X^TX\\omega - X^Ty)_j + \\lambda \\cdot sgn(\\omega^j)=0$$\n",
    "\n",
    "> 其中，$\\lambda \\geq 0$，所以$$\\omega^j=\\omega^{*j}-\\lambda \\cdot sgn(\\omega^j)$$\n",
    "\n",
    "> 可得，$\\omega^j$和$\\omega^{*j}$是同号的，$$\\omega^j=\\omega^{*j}-\\lambda \\cdot sgn(\\omega^j)=sgn(\\omega^{*j})(| \\omega^{*j} |-\\lambda)$$\n",
    "$$|\\omega_{*j}|-\\lambda=|\\omega^j|\\geq 0$$\n",
    "\n",
    "> 最终可得$$\\omega^j=sgn(\\omega^{*j})(|\\omega^{*j}|-\\lambda)_+$$\n",
    "\n",
    "> 其中$(X)_+$表示取$X$的正整数部分，$(X)_+=max(x,0)$\n",
    "\n",
    "> **2、gradient不存在，此时$\\bar{\\omega}^j = 0$**\n",
    "\n",
    "> **定理：** *点$x_o$是凸函数$f$的全局最小值，当且仅当$0\\in \\partial f(x_o)$*\n",
    "\n",
    "> 如果$\\omega^j$ 是最小值，则\n",
    "$$0 \\in \\partial E_{\\hat{\\omega}}=(X^TX\\omega-X^Ty)+\\lambda\\cdot e = \\omega-\\omega^*+\\lambda\\cdot e$$\n",
    "> 其中$e$是一个向量，每一个元素$e^j\\in [-1, 1]$，使得$0=-\\omega^{*j}+\\lambda\\cdot e^j$成立。因此，$$|\\omega^{*j}|=\\lambda|e^j|\\leq \\lambda$$\n",
    "\n",
    ">> 综上所述，可得LASSO Regression的$\\omega$求解公式：$$\\omega^j=sgn(\\omega^{*j}){(|\\omega^{*j}|-\\lambda)}_+$$ \n",
    "\n",
    "> LASSO是一个convex optimization问题，它的优良性质是能产生稀疏性，导致$\\omega$中许多项变成0。\n",
    "\n",
    "\n",
    "> ### LASSO Regression 另解\n",
    "\n",
    "> LASSO的cost function的两部分求解:\n",
    "\n",
    "> 1.RSS部分\n",
    "$$RSS(\\omega)=\\sum^m_{i=1}(y_i-\\sum^n_{j=1}x_{ij}\\omega_j)^2$$\n",
    "> 对$RSS(\\omega)$关于$\\omega$求导：\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial {RSS(\\omega)}} {\\partial{\\omega_k}} &=-2{\\sum^m_{i=1}}x_{ik}{(y_i-\\sum^n_{j=1} x_{ij}\\omega_j)} \\\\\n",
    "&=-2\\sum^m_{i=1}(x_{ik}y_i-x_{ik}\\sum^n_{j=1,j\\neq k}x_{ij}\\omega_j-x^2_{ik}\\omega_k) \\\\\n",
    "&=-2\\sum^m_{i=1} x_{ik}(y_i-\\sum^n_{j=1,j\\neq k}x_{ij}\\omega_j)+2\\omega_k\\sum^m_{i=1}x^2_{ik} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "> 令$p_k=\\sum^m_{i=1} x_{ik}(y_i-\\sum^n_{j=1,j\\neq k}x_{ij}\\omega_j)$，$z_k=\\omega_k\\sum^m_{i=1}x^2_{ik}$，得到：\n",
    "$$\\frac{\\partial {RSS(\\omega)}} {\\partial{\\omega_k}}=-2p_k+2z_k\\omega_k$$\n",
    "\n",
    "> 2.正则项\n",
    "\n",
    ">关于惩罚项的求导使用subgradient(次梯度)\n",
    "\n",
    "$$\\lambda\\frac{\\partial{\\sum^n_{i=1}|\\omega_j|}}{\\partial\\omega_k}=\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "-\\lambda & \\omega_k\\lt0 \\\\\n",
    "[-\\lambda,\\lambda] & \\omega_k=0 \\\\\n",
    "\\lambda & \\omega_k \\gt 0\n",
    "\\end{array} \n",
    "\\right.$$\n",
    "\n",
    "> 整体偏导数：\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial {f(\\omega)}} {\\partial{\\omega_k}} & =-2p_k+2z_k\\omega_k+\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "-\\lambda & \\omega_k\\lt0 \\\\\n",
    "[-\\lambda,\\lambda] & \\omega_k=0 \\\\\n",
    "\\lambda & \\omega_k \\gt 0\n",
    "\\end{array} \n",
    "\\right. \\\\\n",
    "&=\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "2z_k\\omega_k -2p_k -\\lambda & \\omega_k\\lt0 \\\\\n",
    "[-2p_k-\\lambda,-2p_k+\\lambda] & \\omega_k=0 \\\\\n",
    "2z_k\\omega_k -2p_k + \\lambda & \\omega_k \\gt 0\n",
    "\\end{array} \n",
    "\\right. \n",
    "\\end{align}$$\n",
    "\n",
    "> 令$\\frac{\\partial {f(\\omega)}} {\\partial{\\omega_k}}=0$得到\n",
    "$$\\hat{\\omega}=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\frac{(p_k+\\frac{\\lambda}{2})}{z+k} & p_k\\lt -\\frac{\\lambda}{2}\\\\\n",
    "0\\\\\n",
    "\\frac{(p_k-\\frac{\\lambda}{2})}{z+k} & p_k\\gt -\\frac{\\lambda}{2}\n",
    "\\end{array} \n",
    "\\right. $$\n",
    "\n",
    "> Lasso Regression 的损失函数不是连续可导的，由于L1范数用的是绝对值之和，导致损失函数有不可导的点。也就是说，最小二乘法、梯度下降法、牛顿法与拟牛顿法对它统统失效了。对于Lasso Regression采用坐标轴下降法（coordinate descent）和最小角回归法（Least Angle Regression，LARS）求极值解法。　　　　　　\n",
    "\n",
    "> #### 坐标轴下降法（coordinate descent）\n",
    "\n",
    "> 坐标轴下降法是每次选择一个维度的参数进行一维优化，然后不断的迭代对多个维度进行更新直到函数收敛。\n",
    "\n",
    "> 坐标轴下降法顾名思义，是沿着坐标轴的方向去下降，这和梯度下降不同。梯度下降是沿着梯度的负方向下降。不过梯度下降和坐标轴下降的共性就都是迭代法，通过启发式的方式一步步迭代求解函数的最小值。\n",
    "\n",
    "> 坐标轴下降法主要依据结论：一个可微的凸函数$J(\\theta)$, 其中$\\theta$是nx1的向量，即有n个维度。如果在某一点$\\bar{\\theta}$，使得$J(\\theta)$在每一个坐标轴$\\bar{\\theta}_i$(i = 1,2,...,n)上都是最小值，那么$J(\\bar{\\theta}_i)$就是一个全局的最小值。\n",
    "\n",
    "> 于是我们的优化目标就是在$\\theta$的n个坐标轴上(或者说向量的方向上)对损失函数做迭代的下降，当所有的坐标轴上的$\\theta_i$(i = 1,2,...,n)都达到收敛时，我们的损失函数最小，此时的$\\theta$即为我们要求的结果。\n",
    "\n",
    "> 算法过程：\n",
    ">> 1.首先，给$\\theta$向量随机取一个初值。记为$\\theta^{(0)}$ ，上标括号里面的数字代表迭代的轮数，当前初始轮数为0.\n",
    "\n",
    ">> 2.对于第k轮的迭代。从$\\theta^{(k)}_1$开始，到$\\theta^{(k)}_n$为止，依次求$\\theta^{(k)}_i$。$\\theta^{(k)}_i$的表达式如下：\n",
    "\n",
    ">> $$\\theta^{(k)}_i \\in \\underbrace{argmin}_{\\theta_i} J(\\theta^{(k)}_1,\\theta^{(k)}_2,\\cdots,\\theta^{(k)}_{i-1},\\theta_i,\\theta^{(k-1)}_{i+1},\\cdots,\\theta^{(k-1)}_n)$$\n",
    "\n",
    ">> 也就是说$\\theta^{(k)}_i$是使$J(\\theta^{(k)}_1,\\theta^{(k)}_2,\\cdots,\\theta^{(k)}_{i-1},\\theta_i,\\theta^{(k-1)}_{i+1},\\cdots,\\theta^{(k-1)}_n)$最小化时候的$\\theta_i$的值。此时$J(\\theta)$只有$\\theta^{(k)}_i$是变量，其余均为常量，因此最小值容易通过求导求得。\n",
    "\n",
    ">> 具体一点，在第k轮，$\\theta$向量的n个维度的迭代式如下：\n",
    "$$\\begin{align}\n",
    "& \\theta^{(k)}_1 \\in \\underbrace{argmin}_{\\theta_1} J(\\theta_1,\\theta^{(k-1)}_2,\\cdots,\\theta^{(k-1)}_n) \\\\\n",
    "& \\theta^{(k)}_2 \\in \\underbrace{argmin}_{\\theta_2} J(\\theta^{(k)}_1,\\theta_2,\\theta^{(k-1)}_3,\\cdots,\\theta^{(k-1)}_n) \\\\\n",
    "& \\cdots \\\\\n",
    "& \\theta^{(k)}_n \\in \\underbrace{argmin}_{\\theta_n} J(\\theta^{(k)}_1,\\theta^{(k)}_2,\\cdots,\\theta^{(k)}_{n-1},\\theta_n) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    ">>3.检查$\\theta^{(k)}$向量和$\\theta^{(k-1)}$向量在各个维度上的变化情况，如果在所有维度上变化都足够小，那么$\\theta^{(k)}$即为最终结果，否则转入2，继续第k+1轮的迭代。\n",
    "\n",
    "> 以上就是坐标轴下降法的求极值过程，可以和梯度下降做一个比较：\n",
    "\n",
    ">> a) 坐标轴下降法在每次迭代中在当前点处沿一个坐标方向进行一维搜索 ，固定其他的坐标方向，找到一个函数的局部极小值。而梯度下降总是沿着梯度的负方向求函数的局部最小值。\n",
    "\n",
    ">> b) 坐标轴下降优化方法是一种非梯度优化算法。在整个过程中依次循环使用不同的坐标方向进行迭代，一个周期的一维搜索迭代过程相当于一个梯度下降的迭代。\n",
    "\n",
    ">> c) 梯度下降是利用目标函数的导数来确定搜索方向的，该梯度方向可能不与任何坐标轴平行。而坐标轴下降法法是利用当前坐标方向进行搜索，不需要求目标函数的导数，只按照某一坐标方向进行搜索最小值。\n",
    "\n",
    ">> d) 两者都是迭代方法，且每一轮迭代，都需要O(mn)的计算量(m为样本数，n为系数向量的维度)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、Ridge Regression\n",
    "> Ridge Regression表示如下：\n",
    "$$$$\\begin{align}\n",
    "E_{\\hat{\\omega}} &=\\frac{1}{2}(X\\hat{\\omega}-y)^T(X\\hat{\\omega}-y)+\\frac{\\lambda}{2} \\begin{Vmatrix} \\omega \\end{Vmatrix}^2\\\\\n",
    "& =\\frac{1}{2}(\\hat{\\omega}^TX^TX\\hat{\\omega}-2y^TX\\hat{\\omega}+y^Ty)+\\frac{\\lambda}{2} \\begin{Vmatrix} \\omega \\end{Vmatrix}^2\n",
    "\\end{align}$$$$\n",
    "\n",
    "> 对损失函数关于$\\hat{\\omega}$求偏导：\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E_{\\hat{\\omega}}}{\\partial \\hat{\\omega}} &= \\frac{1}{2}(2X^TX\\omega-2X^Ty-0)+ \\lambda \\omega  \\\\\n",
    "&=\\frac{1}{2}(2X^TX\\omega-2X^Ty)+ \\lambda \\omega  \\\\\n",
    "&=X^TX\\omega-X^Ty+ \\lambda \\omega  \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "> 令$\\frac{\\partial E_{\\hat{\\omega}}}{\\partial \\hat{\\omega}}=0$得：\n",
    "$$\\hat{\\omega}=(X^TX+\\lambda I)^{-1}\\cdot X^Ty$$\n",
    "\n",
    "> $X^TX$加上一个单位矩阵是的矩阵变成非奇异矩阵并可以进行求逆运算。\n",
    "\n",
    "> #### 随机梯度下降法(Stochastic Gradient Descent, SGD)\n",
    "\n",
    "> Ridge Regression损失函数关于$\\hat{\\omega}$偏导数存在，故使用随机梯度下降法(Stochastic Gradient Descent, SGD)求解$\\omega$\n",
    "\n",
    "> 为了求解$\\theta=[\\theta_1,\\theta_2,...]$的值，可以先对其赋一组初值，然后改变$\\theta$的值，使得$J(\\theta)$最小。函数$J(\\theta)$在其负梯度方向下降最快，所以只要使得每个参数θ按函数负梯度方向改变，则$J(\\theta)$能最快找到最小值。即 \n",
    "\n",
    "> **代数描述法**\n",
    "$$f(\\omega)=\\frac{1}{m}\\sum^m_{i=1}E_i(\\omega)^2+\\lambda\\sum^n_{j=1}\\omega^2_j$$\n",
    "$$\\frac{\\partial f}{\\partial \\omega_j}(\\omega)=(\\frac{1}{m}\\sum^m_{i=1}2E_i(a)\\cdot x^{(i)}_j )+2\\lambda\\omega_j$$\n",
    "$$\\omega_j:=\\omega_j-\\alpha\\cdot(2E_i(a)\\cdot x^{(i)}_j)+2\\lambda\\omega_j$$\n",
    "\n",
    "> **矩阵描述法**\n",
    "$$\\omega:=\\omega-\\alpha(X^TX\\omega-X^Ty+ \\lambda \\omega)$$\n",
    "\n",
    "> 其中$\\alpha$ 为学习率，即往每次下降最快的方向走多远，一般通过交叉验证选取较好值。\n",
    "\n",
    "> 使用随机梯度下降算法前对数据进行中心化和标准化处理：\n",
    ">>机器学习算法中通常要对原始数据进行中心化和标准化处理，也就是需要将数据的均值调整到0，标准差调整为1, 计算过程是将所有数据减去平均值后再除以标准差；之所以需要进行中心化其实就是个平移过程，将所有数据的中心平移到原点。而标准化则是使得所有数据的不同特征都有相同的尺度Scale, 这样在使用梯度下降法以及其他方法优化的时候不同特征参数的影响程度就会一致了。\n",
    "\n",
    "> **Ridge Regression 的性质：**\n",
    ">> 1、当岭参数 $\\lambda = 0$ 时，得到的解是最小二乘解\n",
    "\n",
    ">> 2、当岭参数 $\\lambda$ 趋向更大时，岭回归系数 $w_i$ 趋向于0，约束项 t 很小\n",
    "\n",
    "> 批量梯度下降（Batch gradient descent）每次迭代都用所有训练样本。随机梯度下降（Stochastic gradient descent，SGD）每次迭代都用一个训练样本，这个训练样本是随机选择的。当训练样本较多的时候，随机梯度下降法比批量梯度下降法更快找到最优参数。批量梯度下降法一个训练集只能产生一个结果。而SGD每次运行都会产生不同的结果。SGD也可能找不到最小值，因为升级权重的时候只用一个训练样本。它的近似值通常足够接近最小值，尤其是处理残差平方和这类凸函数的时候。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、LOSSA & Ridge Regression 的认识\n",
    "> Lasso回归和岭回归最重要的区别是，岭回归中随着惩罚项增加时，所有项都会减小，但是仍然保持非0的状态，然而Lasso回归中，随着惩罚项的增加，越来越多的参数会直接变为0，正是这个优势使得Lasso回归容易用作特征的选择（对应参数非0项），因此Lasso回归可以说能很好的保留那些具有重要意义的特征而去掉那些意义不大甚至毫无意义的特征，而岭回归永远不会认为一个特征是毫无意义的。 \n",
    "\n",
    "> 模型选择的典型方法是正则化。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或罚项(penalty term)。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如，正则化项可以是模型参数向量的范数。\n",
    "\n",
    "> 正则化符合奥卡姆剃刀(Occam's razor)原理。奥卡姆剃刀原理应用于模型选择时变为以下想法：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。可以假设复杂的模型有较大的先验概率，简单的模型有较小的先验概率。\n",
    "\n",
    "> 概括而言就是一句话总结：1范数和0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。然后L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的正则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。\n",
    "\n",
    "> 从贝叶斯角度而言，不同的regularization代表了对权重使用不同的先验分布\n",
    "\n",
    "> Lasso是一阶正则化，岭回归是二阶正则化。Lasso的出发点是减少overfit，而岭回归一般认为是处理多重共线性的一种做法，当然它也有降低overfit的作用。lasso实际上是挑选自变量的一种做法，岭回归是压缩某些系数。二者都会在训练集上放大误差，但是均能在测试集上减小估计误差（理论上）。\n",
    "\n",
    "> 总结：\n",
    ">> 1.默认情况下选用L2正则。\n",
    "\n",
    ">> 2.如若认为少数特征有用，可以用L1正则。\n",
    "\n",
    ">> 3.如若认为少数特征有用，但特征数大于样本数，则选择ElasticNet函数。\n",
    "\n",
    "> 如果发生过拟合， 参数$\\omega$一般是比较大的值，加入惩罚项后，只要控制λ的大小，当λ很大时，θ1到θn就会很小，即达到了约束数量庞大的特征的目的。\n",
    "\n",
    "> 从贝叶斯的角度来分析， 正则化是为模型参数估计增加一个先验知识，先验知识会引导损失函数最小值过程朝着约束方向迭代。 L1正则是Laplace先验，L2是高斯先验。整个最优化问题可以看做是一个最大后验估计，其中正则化项对应后验估计中的先验信息，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计。\n",
    "\n",
    ">  直接使用最小二乘法可能会遇到的问题： \n",
    ">>（1）只有当矩阵X是满秩的时候，才可以用最小二乘法。也就是多个因变量必须是相互独立的，如果相互之间关联较强，或者样本点比较少的时候，很可能造成X就不是满秩的，因为$X^{-1}$是不可逆的。 \n",
    "\n",
    ">>（2）计算大型逆矩阵复杂度高，在处理大规模数据的时候，耗时长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python实现\n",
    "### 1、最小二乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# 没有正则化的最小二乘，该方法返回对应的系数向量\n",
    "def matrix_lstsqr(x, y):\n",
    "    # 构造特征矩阵\n",
    "    X = np.vstack([x, np.ones(len(x))]).T\n",
    "    # 返回系数矩阵\n",
    "    return (np.linalg.inv(X.T.dot(X)).dot(X.T)).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XucVfP+x/HXZ2Zq0gnRTdQ05EhF\noQ51hMipqIcOx13K5WCqSTqo5idCRI4Sasr8KEqHcjvR/aZCSRcR0qkTSkUSlWSmZn9/f+yZnzH2\nNHv2Xvs67+fj4THN3mvW+uwp7/Vd3/X9fpc55xARkeSVEusCREQkshT0IiJJTkEvIpLkFPQiIklO\nQS8ikuQU9CIiSU5BLyKS5BT0IiJJTkEvIpLk0mJdAEDt2rVdZmZmrMsQEUkoq1at2umcq1PednER\n9JmZmaxcuTLWZYiIJBQz+yqY7dR1IyKS5BT0IiJJTkEvIpLkFPQiIklOQS8ikuQU9CIiSU5BLyIS\nrGXL4JFH/F8TSFyMoxcRiXvLlkGHDlBQAFWrwoIF0LZtrKsKilr0IiLBWLTIH/KFhfDLLzBxYqwr\nCpqCXkQkGO3bQ1pRJ4hzMH58wnThlBv0ZjbezHaY2ScB3rvLzJyZ1S763szsKTPbaGYfm9kZkSha\nRCTq2raFG28EM//3hYX+Vn4CCKZF/zzQufSLZtYQ+AuwucTLFwF/LPrvVmBs+CWKiMSJHj2gWjVI\nTfX307dvH+uKglLuzVjn3BIzywzw1hPAAGBaide6AROdcw5438xqmll959x2L4oVEYmptm39N2EX\nLfKHfILcjA1p1I2ZXQJsdc59ZMWXMX7HAVtKfP910WsKehFJDm3bJkzAF6tw0JtZdeAeoGOgtwO8\n5srYz634u3fIyMioaBkiIuFZtszfMq9VC77/PqFa6BUVSou+MXA8UNyabwCsNrMz8bfgG5bYtgGw\nLdBOnHN5QB5A69atA54MREQionhMfH4++HyQkgLp6Qk1Nr4iKjy80jm31jlX1zmX6ZzLxB/uZzjn\nvgHeBHoUjb5pA+xW/7yIRFUws1eLx8T7fP7vfT7/9wkyiqaiym3Rm9lLQHugtpl9DQxxzj1XxuYz\ngYuBjcDPwI0e1SkiUr5gZ6+2b+9/v2SLPoFG0VRUMKNurinn/cwSf3ZAn/DLEhEJQcnZq8Ut9EBB\nX3L0jProRUQSSHFLvbhFf6gWegKOngmVgl5EkkeCjnOPNAW9iCSXSLfUi4dlJtCJREEvIhKsBF2q\nWKtXiogEK9DN3gSgoBcRCVbxzd5kW9RMRESKJOjNXgW9iEhFJOCwTHXdiIgkOQW9iEiSU9CLSHwK\nZnEyCYr66EUk/iToePV4pRa9iERfea31BB2vHq/UoheR6AqmtV6RxcmkXGrRi0h0BdNaLx6vPnRo\n+d028d6XHwf1qUUvItEVbGs9mPHq8d6XHyf1qUUvIsHzonVakdZ6eeKwL39P/h627y16gmqc1KcW\nvYgEx8vWqVezS+OoL985x2vrXqPf7H60rNeSmdfNjJv6FPQiEpxgH9MXbT17+r/26BGzer744Quy\nZ2Uzc8NMTjvmNIacN8T/RpysjaOgF5HgxEnr9P+VvsLo0SPqJRQUFjBi6QgeXPIgaSlpjOw4kr5n\n9SUtpUS0xsHaOAp6EQlOnLRO/1+MrzAWf7mYXjN6sW7nOi5rehlPdn6SBkc0iNrxK0JBLyLB87p1\nGs5j+WJ0hbFj3w7unnc3Ez+aSGbNTGZcO4OL/3hxVI4dKgW9iIQvlMAO9+ZulK8wfM7Hc6ufY+D8\ngewt2Mugswdx73n3Ur1K9Yge1wsKehEJT6iB7UXXS5T6v9d+u5asGVks3bKUczLOYVzXcTSr0yzi\nx/WKxtGLSHhCHSueAI/l+6ngJ+6eezenP3M663euZ0K3CSy+YbF3IR+lWbNq0YtIeELtK4+3m7ul\nvLn+TfrO6svm3Zu56bSbeOwvj1Grei3vDhDFWbMKehEJTziBHQdDD0vbvHszt8+6nWnrp3FK3VN4\n98Z3OTvjbO8PFMVRQwp6EQlfpAM7nNE5QTpQeIBR74/i/sX3AzD8wuH0b9OfKqlVInK8aI4aUtCL\nSHyLQhfH0i1LyZqexdoda7mkySU81fkpGtVs5OkxfieKXVcKehGJbxHs4ti1fxcD5w3k2Q+fpeER\nDXnjqjf468l/9WTfQYlS15WCXkTiWwS6OJxzTPp4EnfOvZMf9v/AXW3vYkj7IdSoWiPsfccjBb2I\nxDevujiK+vm/bJnJDTueYfFXi2nboC3juo6jRb0WXlYcdxT0IhL/wu3iWLYM16EDvvxfqJviSL/l\ncJ65+Rn+fsbfSTGPphNF4YZxqBT0IpL0/vNaHif8sp80B+kYr9XrS41Wt3p3gDh5klRZyj2Vmdl4\nM9thZp+UeO2fZva5mX1sZm+YWc0S7+WY2UYzW29mnSJVuIhIebbt3caVr1xJzx+f50Ca4UtNITW9\nGjU6dv11Iy9mp8bJk6TKEkyL/nlgNDCxxGvzgBzn3EEzGw7kAAPNrBlwNdAcOBaYb2YnOecKvS1b\nRKRsB30HyV2Ry+CFgzngO8C9PR8m7YZ2pLzz3m+7VrxqicfbWv2llBv0zrklZpZZ6rW5Jb59H7i8\n6M/dgJedc/nAF2a2ETgTiNPHs4tIslmxdQVZM7JYvX01nU/szJiLx3DCUSf432x37m839mroZpwv\n5+BFH/1NwJSiPx+HP/iLfV302u+Y2a3ArQAZGRkelCEildmPv/zI4IWDyV2RS/3D6/PKFa/wt6Z/\nw8zK/iEvW+JxuJxDsbBuN5vZPcBBYHLxSwE2c4F+1jmX55xr7ZxrXadOnXDKEJFKzDnHS2tf4uTR\nJzN25Viyz8xmXZ91XL77OOzRRw/d917cEh86NO5uoHop5Ba9mfUEugIdnHPFYf410LDEZg2AbaGX\nJyJStg3fb6DPzD7M2zSP1se2ZuZ1Mzmj/hkV63uP45a4V0Jq0ZtZZ2AgcIlz7ucSb70JXG1m6WZ2\nPPBH4IPwyxQRz0VpLfRIyD+YzwOLHuDUsaeyfOtyRl80mvdvft8f8hD3o2CirdwWvZm9BLQHapvZ\n18AQ/KNs0oF5Rf1f7zvnspxzn5rZVOAz/F06fTTiRiQOxfm470OZv2k+vWf0ZsOuDVx9ytWM7DiS\n+ofX/+1GXvW9x/EkqIoIZtTNNQFefu4Q2z8MPBxOUSISYYsWQX4++Hz+rxFcC90r3/z0DXfOvZN/\nrf0XjY9qzJzuc+jYuGPgjb0YBZPAJ8PSNDNWpDKqVcsf8uD/WqvUk5PiqCVb6Cskb1UeOQty2H9w\nP/edex855+RQLa3aoX8w3L73KD4YJNIU9CKV0fffQ0qKP+RTUvzfF4ujluyH2z8ka0YWH2z9gA7H\ndyC3Sy4n1TopOgeP80lQFaGgF6mM2reH9PTAIRZqS9bDq4C9+Xu57+37eOqDp6hdvTYvXvoi1556\n7aHHxHstzidBVYSCXqQyOlSIhdKS9egqwDnHa+te447Zd7Bt7zZ6te7Fwx0epma1muX/cCQkydBL\nBb1IIgunFV1WiIXSkvWgP3vTD5vInpnNrI2zOO2Y03jtytc4q8FZFdqHBKagF0lUkexLr2hLNoz+\n7ILCAh5f+jhDlwwlLSWNJzo9QfaZ2aSlKJ68ot+kSKKK1KiQUK4SQuzPXvzlYnrN6MW6nev4W9O/\nMarzKBoc0SDEwqUsCnqRRBWJUSHhXCVU4Crgu33fcfe8u3nhoxfIrJnJ9Gum0+WkLmEULoeioBdJ\nVJEYFRLhseM+52P8h+MZOH8ge/L3kNMuh8HnDqZ6leqeHUN+T0Evksi8HhUSwbHja79dS9aMLJZu\nWco5GecwtstYmtdt7tn+pWwKehH5VQSuEvYV7OOBxQ8wctlIalaryYRuE+jZsmd0x8RXcgp6Efmt\nUK8SAtzEfXP9m/Sd1ZfNuzdz8+k3M/zC4dSqXuuQuxHvKehFJHylbuJ+M20yWd+/wLT102hepznv\n3PgO7TLaxbrKSktBLyLhK3ETtzD/F8Y+fhXzzqvCYxc+xh1t7qBKapVYV1ipKehF4kUcrRhZYe3b\nU1glDecKKUhx/PznM/ms92Qa1WwU68oEBb1IfIijFSMratf+XQzaOYG11+Xz1+1H0rZ7Dv+8ZmCs\ny5ISFPQi8SAB1z53zjHp40ncNfcudu3fxR1X3kmf9vdTo2qN6BURzFVQIl8peURBLxIPEmzt83Xf\nraP3zN4s+nIRbRu0ZVzXcbSo1yK6RQRzFZTAV0peCunh4CLiseLx60OHxnUY7T+wn8ELB9NyXEs+\n+uYj8rrm8e5N70Y/5CG4B4DrIeGAWvQi8SMaa5+H0Y0xe+Nsnh1zM39cu43BF3YmK/sF6v6hbkTK\nDEpZV0ElP2OCXSlFioJeJByJ1P8bYjfG1j1b6T+nP1vmvMLCiUZ6YQopSxfDef+FtjEM+kCzeAN9\nxiR5SlQ4FPQioUq0/t8K3vA96DtI7opcBi8czAHfAWakXEg139uYL4wbxl6fGEtfBQX6jDk58f33\nEgUKepFQJdpImQp0Y6zYuoKsGVms3r6azid2ZvRFo2m8fge82CH0bpBonBjVVROQgl4kVIkWKkEs\nWPbjLz8yeOFgclfkckyNY5h6+VQub3a5fwGyto3D6waJxokxiR7o7SUFvUioEjFUyrjh65xjyqdT\n6D+nPzv27SD7zGweuuAhjkg/IqifL9eyZbB5M6Sm+r+P5IkxSR7o7SUFvUg4kiBUNny/gT4z+zBv\n0zxaH9uaGdfO4Iz6Z3h3gJJdNmlpcMst0KNHwv/eEomCXiRZlXPj85eDvzD83eE88u4jpKelM/qi\n0WS1ziI1JdXbOkp22QBkZCjko0xBL5KMyrnxOX/TfHrP6M2GXRu4+pSrGdlxJPUPrx+ZWhLtXkYS\nUtCLJKMybnx+89M33Dn3Tv619l80Pqoxc7rPoWPjjpGtJRHvZSQZBb1IJMR6IlWpVnThueeQt2Is\nOQty2H9wP/edex855+RQLa1adOpJgnsZiUxBL1LMq3COh4lUJVrR60+pT491d/LB1g/ocHwHcrvk\nclKtk6Jbj8SUgl4Eyg7nUMI/TiZS7T3jFO7bPZWnPhhM7eq1efHSF7n21Gv1UO5KSEEvAmWvchhK\nyzzYm48R6t5xzvH6utfpN7sf2/Zu47ZWtzGswzCOOuwoz44hiaXcoDez8UBXYIdz7pSi144GpgCZ\nwJfAlc65H8zfVHgSuBj4GbjBObc6MqWLeChQOIfaMg/m5mOEune++OELsmdlM3PDTFrWa8mrV75K\nmwZtwt6vJLZgWvTPA6OBiSVeGwQscM49amaDir4fCFwE/LHov7OAsUVfReJbWeEc6rDA8m4+ety9\nU1BYwIilI3hwyYOkpaTxRKcnyD4zm7QUXbRLEEHvnFtiZpmlXu4GtC/68wvAIvxB3w2Y6JxzwPtm\nVtPM6jvntntVsEjElA7nSA4L9HBs+ZKvlpA1PYt1O9dxWdPLGNVpFA2PbOhZqZL4Qj3d1ysOb+fc\ndjMrXpT6OGBLie2+LnpNQS+xE05feKSGBXpwEvlu33cMmD+A59c8T2bNTKZfM50uJ3XxvFRJfF5f\n1wW6ne8Cbmh2K3ArQEZGhsdliBSJh6GOZQnxJOJzPiZ8OIEB8wewJ38POe1yGHzuYKpXqR6BIiUZ\nhPrM2G/NrD5A0dcdRa9/DZS8ZmwAbAu0A+dcnnOutXOudZ06dUIsQ6QcSfbM0E92fMK5E87l72/9\nneZ1mrPmtjUM6zBMIS+HFGrQvwn0LPpzT2Baidd7mF8bYLf65yWmivvCU1MTep2VfQX7GDhvIKc/\nczqf7/ycCd0msPiGxTSv2zzWpUkCCGZ45Uv4b7zWNrOvgSHAo8BUM7sZ2AxcUbT5TPxDKzfiH155\nYwRqFglePK6zUsF7Bm+tf4vsWdls3r2Zm0+/mUcvfJTa1WtHvExJHuYfIBNbrVu3ditXrox1GSKR\nV4F7Bpt3b6bf7H78+/N/07xOc8Z1HUe7jHZRLljimZmtcs61Lm+7ULtuRCQUQdwzOFB4gMeXPk6z\nMc2Ys3EOwy8czoe3fRg45Jctg0ce8X8VKYNmU4hEUznj55duWUruU9fT8MNNZJ1zNtn9XiSzZmbg\nfcXziCKJKwp6kWgq457Brv27yJmfw8fT8lg40UgvTMGWrsY6boe2mYH3Fczs2lgvlyxxQUEvEm0l\nxs8755j08STumnsXu/bv4k1fO6r5lmG+IJZGKG92rVr8UkR99CIxsu67dVww8QJ6/rsnJx59Iqtv\nW83FtzyGBTsctPjqYOjQwCGeZHMIJHRq0YtE2f4D+3n4nYd57L3HqFG1Bnld87j5jJtJsRSoR8WG\ngx5qdq2e1SpFFPQiUTRrwyyyZ2Wz6YdNXN/ieh7v+Dh1/1D3txuVDu9Q+9njcQ6BxISCXiQKtu7Z\nSv85/Xnls1doUqsJC3ss5Pzjzy//B8PtZ9ezWgUFvUhEHfQdJHdFLoMXDuaA7wBDzx/K3X++m/S0\n9OB2ECePJZTEpqAXiZAVW1eQNSOL1dtX06lxJ8ZcPIbGRzeu2E7Uzy4eUNCLeOzHX35k8MLB5K7I\n5ZgaxzDl8ilc0eyK0B7KrX528YCCXsQjzjmmfDqF/nP6s2PfDrLPzOahCx7iiPQj4vPhJ1JpKOhF\nKipAaG/4fgN9ZvZh3qZ5tD62NdOvmU6rY1v9ur0mLkkMKehFKqJUaBfMmcWjvsUMe2cY6WnpPH3R\n0/Rq3YvUlNRff0Y3VCXGFPQiFVEitH0F+Tz9z8sZ0monV59yNSM7jqT+4fV//zOxuKGqNW6kBAW9\nREewwRPvAdW+Pa5qFXz5PvLNx/uN05nTfQ4dG3cs+2eifUNVXUVSioJeIi/Y4ImHgDrEica39D1W\nTP4nkzvBET+l0KhbDyZl5VItrVr5+/XihmqwJ0F1FUkpCnqJvGCDJ9YBdYgTzX+mv0DGZTfR6qCP\nllVS+Pbfk2l00dVxUdvvaOy9lKLVKyXygn1Ad6wf5B3gRLM3fy//mPMPnh91I2kHfaQ5SC80Gq35\nIua1lam8VS2l0lGLXiIv2D7qWE8OKtESdlWrsjgTuo9pyra923i0czdSl86BggL/MsLRPglVtJWu\nsfdSgh4OLlLSsmXsmvU6w1KXMoKltKzXknFdx9GmQZvY3yiO9fEl7gT7cHC16EWKFBQWMOLgIoam\njyHFUhh5/kj6ntWXtJSi/01i3UqO9fElYSnoRYAlXy0ha3oW63au47KmlzGq0ygaHtkw1mWJeEJB\nL5Xazp93cve8u3l+zfM0OrIRb13zFl1P6hrrskQ8paCXyInjPmWf8zHhwwkMmD+APfl7GHT2IO49\n716qV6ke69JEPKegl9/zIqDjYfJTGT7Z8QlZ07N4b8t7nJNxDmO7jKV53ebh7TSOT2oiCnr5La8C\nOtaTnwLYV7CPBxc/yMj3R3Jk+pFM6DaBni17hrZOfElxfFITAU2YktIqMjHnUGI9+amUN9e/SbPc\nZjy29DF6tuzJ+uz13HDaDeGHPHj3OxOJELXo5be8mj4f68lPRTbv3szts25n2vppNK/TnHdufId2\nGe28PYiWHJA4pwlT8ntJ0N98oPAATy5/kvsX3Y/P+bi//f30b9OfKqlVInPAJPidSeIJdsKUgl6S\nztItS8mansXaHWvpelJXnr7oaTJrZsa6LBHPaWaseC/OW6279u8ib/SN7J7zJsc3r8ODvd6gW5Nu\n3vTDiyQwBb0EJ1IjSzw4eTjnmPTxJF7+3368+syPpPuMlKU/YVfXg+KQj/OTlEgkKeglOJEYLunB\nyWPdd+voPbM3i75cRO72hhzm9mA+329rXLYMzj//1+O8/bbCXiqVsIZXmll/M/vUzD4xs5fMrJqZ\nHW9my81sg5lNMbOqXhUrMRSJ4ZJhDEvcf2A/gxcOpuW4lqz5Zg3PdH2G2+58Caua/vsaJ06E/Hxw\nzv914sTwaxdJICG36M3sOOB2oJlzbr+ZTQWuBi4GnnDOvWxm44CbgbGeVCuxE4nhkiEOS5y9cTZ9\nZvZh0w+b6N6iOyM6jqDuH+r634yDIZ0i8SbkUTdFQf8+0BLYA/wbeBqYDBzjnDtoZm2B+51znQ61\nL426qcQq0He+be82+s/pz9RPp9KkVhPGdhnL+cefH9wx2reHAwegSpW4mKUr4oWIj7pxzm01s8eB\nzcB+YC6wCvjROXewaLOvgeNCPYZUAsWBW9xtEyCAC32F5K7I5Z6F91BQWMCD7R9kwNkDSE9LD/4Y\nixappS+VVjhdN0cB3YDjgR+BV4CLAmwa8JLBzG4FbgXIyMgItQxJdOXckF335ngWTLiXf9Xaxp/b\nd2LMxWNofHTj3/58MAGuh3ZIJRbOqJsLgS+cc98BmNnrwJ+BmmaWVtSqbwBsC/TDzrk8IA/8XTdh\n1CGJrIzRPLt/2c2zY26m16DXyCqErPSqpN50H1Y65LWYmEi5whl1sxloY2bVzT8jpQPwGfA2cHnR\nNj2BaeGVKEmt1Gged955vPzJy5w85mS+n/U66T4jzUHagUJs8eLf/qwWExMJSshB75xbDrwKrAbW\nFu0rDxgI/MPMNgK1gOc8qFOSVfFonqFD+fqNF+i86QGuee0ajjv8OK6//VlS06uVPaQz3CGfy5bB\nI4/4v4oksbAmTDnnhgBDSr28CTgznP1K5VJwsIB3vljAg18MYc0Jh/FU56fo/afepKakwoKmZffB\nhzPkU90+UoloZqzE1IpXn+aUa/tx3kHH3Cqp7JkxiTpnXfLrBuXdRA31JmscPhhFJFL04BGJiW9/\n+pbur3fn9XG3U+WgI81BeiHUWfFpdAqIswejiESSgl6i2lftcz7GrRxHk9FNeOWzVzjhrzeSWu2w\n6AduiXsD6raRZKeum8ouin3Va75ZQ9b0LJZvXc4Fx19A7sW5NKndBFrdEpvJTBpbL5WEgj4ZVWRJ\n3ij0Ve/N38uQRUN4cvmT1K5em0mXTuK6U6/7dZ348gK3rM+jpYdFgqKgTzYVbaFH8HmnzjleX/c6\n/Wb3Y9vebdzW6jaGdRjGUYcdFfxOyvo8GjUjEjT10Sebik4iilBf9Rc/fEHXl7py+SuXU7t6bZbe\nvJSxXcdWLOSh7M+jyVIiQVOLPtmE0kIvq+skhK6RgsICRiwdwdAlQ0mxFEZ2HEnfs/qSlhLiP7Wy\nPk8Er0REko2CPhn17On/2qNH6C30ELpGlny1hKzpWazbuY7Lml7GqE6jaHhkw9COX6ysSVGRWB9f\nJEkp6JNJ6XDu0SP0fVXgJu13+75jwPwBPL/meRod2Yi3rnmLrid1Df3YpZV1xaFRMyJBUdAnEy9H\n0ATRNeJzPiZ8OIEB8wewJ38Pg84exL3n3Uv1KtXD+BAi4jUFfaIr2Y/uZb91OV0jn+z4hKzpWby3\n5T3aZbRjXJdxNK/bPPTjiUjEKOgTWaB+dC/7rQN0jexfspBFzz/AsNT3+PyPNRl/yXh6ntaTFNMA\nLpF4paCPlGhM5gnUVZOTE7HjvfvyP2nVYyB/Oei4oEoq+2dPoubpAR4qpolMInFFQR8J0ZrME6Uh\nhlt2b+H22bdz8rP/ps1B/A8CKYT099fA+aWCXhOZROKOrrcjIVqTeSK8MNeBwgOMWDqCpmOaMmfj\nHJpf0bv8Bcg0kUkk7qhFHwnRnMwToSGGy7YsI2tGFh9/+zFdT+rK0xc9TWbNTGjR/dDdMprIJBJ3\nzLnYP5e7devWbuXKlbEuw1te91NHqd971/5d5MzPIW91Hg2OaMDTFz1Ntybdfl2ALBjqoxeJCjNb\n5ZxrXe52CvoEEIl+71Jh7JzjxY9f5M65d7Jr/y76ndWP+9vfz+Hph3vxCUQkAoINenXdxLPiMN68\n2dulhEudOL587Tlu2vG/vP3l27Rp0IZ5XebR8piWXn0KEYkxBX28KhnGqamQVvRX5UW/d4kbpoX5\nv/DsyO58eOERjOsyjlta3aIx8SJJRkEfr0qOXgG45RbIyPCm37t9ewqrpOFcIQUpjiodOvJ5nxeo\nV6NeuFWLSBxS0Mer0qNXwlmJsoRte7fRf+soNl+XzxU7anNuz/sY8re+4dcrInFLQe8Vr0eaeLwM\nb6GvkNwVudyz8B4KCgu45/oH6XP2ANLT0sOvVUTimoLeC5GaDerRGPmV21aSNT2LVdtX0alxJ0Zf\nPJoTjz4x/PpEJCHorpsX4nQ26O5fdtN3Zl/O/N8z2bZ3G1Mun8Ks62Yp5EUqGbXovRBns0Gdc0z9\ndCp3zLmDHft2kH1mNg9d8BBHpB8R07pEJDYqR9BHeqZmHD3WbuOujfSZ2Ye5/51Lq/qteOuat2h9\nbID5FMH8TjTDVSQpJH/QR2s1xRg/1i7/YD7D3xvOsHeGkZ6WzlOdn6L3n3qTmpL6+42D+Z1oFUqR\npJH8ffRx2n/upQWbFtBiXAuGLBrCpU0v5fM+n9P3rL6BQx6C+51Ugt+bSGWR/C36OOs/99K3P33L\nnXPvZPLayTQ+qjGzr5tNpxM7lf+DwfxOkvj3JlLZJH/Qx1H/uVd8zkfeqjwGzR/Ezwd+5t5z7yWn\nXQ6HVTksuB0E8ztJwt+bSGWl1SsTzJpv1pA1PYvlW5dzfub55HbJ5eTaJ8e6LBGJAa1emWT25u9l\nyKIhPLn8SWodVotJl07iulOvq9g68SJSKYUV9GZWE3gWOAVwwE3AemAKkAl8CVzpnPshrCorMecc\nb3z+BrfPup2te7dyW6vbeKTDIxx12FEV25GGSopUWuG26J8EZjvnLjezqkB14H+ABc65R81sEDAI\nGBjmcSqlL3/8kuyZ2czYMIMW9VrwyhWv0LZhCCGtoZIilVrIwyvN7AjgXOA5AOdcgXPuR6Ab8ELR\nZi8Afw23yMqmoLCAR999lGZjmrHoy0WM6DiCVbeuCi3kQUMlRSq5cFr0JwDfARPMrCWwCugH1HPO\nbQdwzm03s7qBftjMbgVuBcjIyAijDI/FuIvjna/eIWtGFp999xmXNb2MUZ1G0fDIhuHtVEMlRSq1\ncII+DTgD6OucW25mT+LvpgmKcy4PyAP/qJsw6vBODLs4dv68kwHzBjBhzQQu/eEYXk3tQdOGWRBu\nyIOGSopUcuHMjP0a+No5t7x4WS83AAAHYklEQVTo+1fxB/+3ZlYfoOjrjvBKjKKKdHEsWwaPPOL/\nWpH3SvE5H+M/HM/Jo09m0seTyD3qel7L203TJyf7Tzol91GB/f5O27aQk6OQF6mEQm7RO+e+MbMt\nZtbEObce6AB8VvRfT+DRoq/TPKk0EK+7WYLt4sjLgz59wOeD9PTftvwrcFXw6Y5PyZqRxbub36Vd\nRjvGdRlH8+feDPwgcN1QFZEQhTvqpi8wuWjEzSbgRvxXCVPN7GZgM3BFmMcILFLB17On/2tZj+7L\ny4NevfwhD5Cf/2sYQ+CrglL72Vewj6FLhjJi2QiOTD+S8ZeMp+dpPf0P5W6/J/DJJoj9iogEElbQ\nO+fWAIFmZXUIZ79B8Tr4Sp84evQIvE1xS75YSspvW/7FVwX5+WAGtWr9ZhfT/zOd7JnZfLX7K246\n7SaG/2U4tavX/nWDsvrTdUNVREKUuDNjvQ6+YE4cixb9PuTHjPntdm3bwqhRkJ3t39cddwDw49b/\n8mja+wz3LaFZnWYsuWEJ5zQ6J3AtgZY81g1VEQlR4ga918EX7IqO6en+1npxyN966++3+/57/wnB\n58Pl5+Pr3YsaPh/3pUHzp3px1S2jqJpateI1xnjNexFJTIkb9OBt8Hm5omPRScMV5HPQ+TAfpDlI\n9aVy/Q8NIZSQFxEJUWIHvdeCOXEEsc0Pp51M3oMd2T17GgePrsmwt37GHSjE1LcuIjGgoPeQc47J\nayfzjzn/YNf+XfS7+x88cP4DpK1aq751EYkZBX0wghiv//nOz+k9ozdvf/k2bRq0YV6XebQ8pqX/\nTfWti0gMKejLU854/f0H9jPsnWEMf284f6j6B8Z1GcctrW7xj4kXEYkDCvryHGLY5eyNs+kzsw+b\nfthE9xbdefwvj1OvRr2YlisiUpqCvjwBhl1u27uN/nP6M/XTqTSp1YQFPRZwwfEXxLpSEZGAFPTl\nKTGksvDcc8hNWck9oztRUFjAg+0fZMDZA0hPS491lSIiZVLQB6NtW1Y2qkLW9CxWbV9Fx8YdGXPx\nGE48+sRYVyYiUi4FfTl2/7KbwQsHM2bFGOrVqMfLf3uZK5tfqYdyi0jCUNCXwTnHlE+n0H9Of779\n6Vv6/KkPD13wEEdWOzLWpYmIVIiCPoCNuzbSZ2Yf5v53Lq3qt+Kta96i9bGBFukUEYl/CvoS8g/m\n89h7j/HwOw9TNbUqT3V+it5/6k1qSqp3B4nxM2lFpPJR0Jdw1atXMW39NK5qfhUjO43k2MOP9fYA\nekqUiMSAgr6EgWcPpFfrXnQ6sVNkDqCnRIlIDCjoS2jbMMKhq6dEiUgMKOijSU+JEpEYUNBHm1ay\nFJEo0xKLIiJJTkEvIpLkFPQiIklOQS8ikuQU9CIiSU5BLyKS5Mw5F+saMLPvgK/KeLs2sDOK5URb\nMn8+fbbElcyfL5k+WyPnXJ3yNoqLoD8UM1vpnEvapSOT+fPpsyWuZP58yfzZyqKuGxGRJKegFxFJ\ncokQ9HmxLiDCkvnz6bMlrmT+fMn82QKK+z56EREJTyK06EVEJAwJE/Rm1tfM1pvZp2b2WKzriQQz\nu8vMnJnVjnUtXjGzf5rZ52b2sZm9YWY1Y11TuMysc9G/xY1mNijW9XjFzBqa2dtmtq7o/7N+sa7J\na2aWamYfmtn0WNcSTQkR9GZ2PtANaOGcaw48HuOSPGdmDYG/AJtjXYvH5gGnOOdaAP8BcmJcT1jM\nLBUYA1wENAOuMbNmsa3KMweBO51zTYE2QJ8k+mzF+gHrYl1EtCVE0AO9gEedc/kAzrkdMa4nEp4A\nBgBJddPEOTfXOXew6Nv3gQaxrMcDZwIbnXObnHMFwMv4GyEJzzm33Tm3uujPe/EH4nGxrco7ZtYA\n6AI8G+taoi1Rgv4k4BwzW25mi83sT7EuyEtmdgmw1Tn3UaxribCbgFmxLiJMxwFbSnz/NUkUhsXM\nLBM4HVge20o8NQp/Y8oX60KiLW6eMGVm84FjArx1D/46j8J/OfknYKqZneASaMhQOZ/vf4CO0a3I\nO4f6bM65aUXb3IO/a2ByNGuLAAvwWsL8OwyGmdUAXgPucM7tiXU9XjCzrsAO59wqM2sf63qiLW6C\n3jl3YVnvmVkv4PWiYP/AzHz416v4Llr1hausz2dmpwLHAx+ZGfi7Nlab2ZnOuW+iWGLIDvV3B2Bm\nPYGuQIdEOjmX4WugYYnvGwDbYlSL58ysCv6Qn+ycez3W9XjobOASM7sYqAYcYWYvOue6x7iuqEiI\ncfRmlgUc65y7z8xOAhYAGUkQGr9jZl8CrZ1zSbHokpl1BkYC5znnEubEXBYzS8N/U7kDsBVYAVzr\nnPs0poV5wPwtjReAXc65O2JdT6QUtejvcs51jXUt0ZIoffTjgRPM7BP8N796JmPIJ6nRwOHAPDNb\nY2bjYl1QOIpuLGcDc/DfrJyaDCFf5GzgeuCCor+rNUUtYElwCdGiFxGR0CVKi15EREKkoBcRSXIK\nehGRJKegFxFJcgp6EZEkp6AXEUlyCnoRkSSnoBcRSXL/B2zKpL9tEDdyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106eab6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([  8.,  90.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# 生成 x\n",
    "x = np.arange(-5, 5, 0.1)\n",
    "# y = 8*x+90\n",
    "y = [(8*a + 90) for a in x]\n",
    "\n",
    "# 生成拟合图像\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, y, color='g', linestyle='-', marker='')\n",
    "\n",
    "x_a = [b1*(random.randint(90,120))/100 for b1 in x]\n",
    "y_a = [b2*(random.randint(90,120))/100 for b2 in y]\n",
    "ax.plot(x_a, y_a, color='r', linestyle='', marker='.')\n",
    "plt.show()\n",
    "\n",
    "# 通过最小二乘法求解 y = 8x+90 的系数\n",
    "matrix_lstsqr(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、LASSO Regression实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    ''' \n",
    "        加载数据\n",
    "    '''\n",
    "    X, Y = [], []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            splited_line = [float(i) for i in line.split()]\n",
    "            x, y = splited_line[: -1], splited_line[-1]\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "    X, Y = np.matrix(X), np.matrix(Y).T\n",
    "    return X, Y\n",
    "\n",
    "def standarize(X):\n",
    "    ''' \n",
    "        中心化 & 标准化数据 (零均值, 单位标准差)\n",
    "    '''\n",
    "    std_deviation = np.std(X, 0)\n",
    "    mean = np.mean(X, 0)\n",
    "    return (X - mean) / std_deviation\n",
    "\n",
    "def std_linreg(X, Y):\n",
    "    xTx = X.T*X\n",
    "    if np.linalg.det(xTx) == 0:\n",
    "        print('xTx is a singular matrix')\n",
    "        return\n",
    "    return xTx.I*X.T*Y\n",
    "\n",
    "def get_corrcoef(X, Y):\n",
    "    # X Y 的协方差\n",
    "    cov = np.mean(X*Y) - np.mean(X)*np.mean(Y)\n",
    "    return cov/(np.var(X)*np.var(Y))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from math import exp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def lasso_regression(X, y, lambd=0.2, threshold=0.1):\n",
    "    ''' \n",
    "        通过坐标下降(coordinate descent)法获取LASSO回归系数\n",
    "    '''\n",
    "    # 计算残差平方和\n",
    "    rss = lambda X, y, w: (y - X*w).T*(y - X*w)\n",
    "    # 初始化回归系数w.\n",
    "    m, n = X.shape\n",
    "    w = np.matrix(np.zeros((n, 1)))\n",
    "    r = rss(X, y, w)\n",
    "    # 使用坐标下降法优化回归系数w\n",
    "    niter = itertools.count(1)\n",
    "    for it in niter:\n",
    "        for k in range(n):\n",
    "            # 计算常量值z_k和p_k\n",
    "            z_k = (X[:, k].T*X[:, k])[0, 0]\n",
    "            p_k = 0\n",
    "            for i in range(m):\n",
    "                p_k += X[i, k]*(y[i, 0] - sum([X[i, j]*w[j, 0] for j in range(n) if j != k]))\n",
    "            \n",
    "            # 分情况计算w_k\n",
    "            if p_k < -lambd/2:\n",
    "                w_k = (p_k + lambd/2)/z_k\n",
    "            elif p_k > lambd/2:\n",
    "                w_k = (p_k - lambd/2)/z_k\n",
    "            else:\n",
    "                w_k = 0\n",
    "            w[k, 0] = w_k\n",
    "        r_prime = rss(X, y, w)\n",
    "        delta = abs(r_prime - r)[0, 0]\n",
    "        r = r_prime\n",
    "        # print('Iteration: {}, delta = {}'.format(it, delta))\n",
    "        if delta < threshold:\n",
    "            break\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "系数矩阵 [[ 0.01491625]\n",
      " [ 0.        ]\n",
      " [ 0.36007948]\n",
      " [ 0.15395325]\n",
      " [ 1.00970149]\n",
      " [-1.23437005]\n",
      " [-0.22746198]\n",
      " [ 0.50638396]]\n",
      "Correlation coefficient: 0.7255254877587117\n"
     ]
    }
   ],
   "source": [
    "# 验证LASSO的坐标下降算法\n",
    "X, y = load_data('abalone.txt')\n",
    "X, y = standarize(X), standarize(y)\n",
    "w = lasso_regression(X, y, lambd=10)\n",
    "y_prime = X*w\n",
    "print(\"系数矩阵\", w)\n",
    "\n",
    "# 计算相关系数\n",
    "corrcoef = get_corrcoef(np.array(y.reshape(1, -1)), np.array(y_prime.reshape(1, -1)))\n",
    "print('Correlation coefficient: {}'.format(corrcoef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lasso_traj(X, y, ntest=30):\n",
    "    ''' \n",
    "        获取回归系数轨迹矩阵\n",
    "    '''\n",
    "    _, n = X.shape\n",
    "    ws = np.zeros((ntest, n))\n",
    "    for i in range(ntest):\n",
    "        w = lasso_regression(X, y, lambd=exp(i-10))\n",
    "        ws[i, :] = w.T\n",
    "        # print('lambda = e^({}), w = {}'.format(i-10, w.T[0, :]))\n",
    "    return ws\n",
    "\n",
    "ntest=30\n",
    "# 绘制轨迹\n",
    "ws = lasso_traj(X, y, ntest= 30)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "lambdas = [i-10 for i in range(ntest)]\n",
    "ax.plot(lambdas, ws)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、Ridge Regression\n",
    "#### 矩阵计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(X, y, lambd=0.2):\n",
    "    ''' \n",
    "        获取岭回归系数\n",
    "    '''\n",
    "    XTX = X.T*X\n",
    "    m, _ = XTX.shape\n",
    "    I = np.matrix(np.eye(m))\n",
    "    w = (XTX + lambd*I).I*X.T*y\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntest = 30\n",
    "# 加载数据\n",
    "X, y = load_data('abalone.txt')\n",
    "\n",
    "# 中心化 & 标准化\n",
    "X, y = standarize(X), standarize(y)\n",
    "\n",
    "# 测试数据和训练数据\n",
    "w_test, errors = [], []\n",
    "for i in range(ntest):\n",
    "    lambd = exp(i - 10)\n",
    "    # 训练数据\n",
    "    X_train, y_train = X[: 180, :], y[: 180, :]\n",
    "    # 测试数据\n",
    "    X_test, y_test = X[180: -1, :], y[180: -1, :]\n",
    "\n",
    "    # 岭回归系数\n",
    "    w = ridge_regression(X_train, y_train, lambd)\n",
    "    error = np.std(X_test*w - y_test)\n",
    "    w_test.append(w)\n",
    "    errors.append(error)\n",
    "\n",
    "# 选择误差最小的回归系数\n",
    "w_best, e_best = min(zip(w_test, errors), key=lambda x: x[1])\n",
    "print('Best w: {}, best error: {}'.format(w_best, e_best))\n",
    "\n",
    "y_prime = X*w_best\n",
    "# 计算相关系数\n",
    "corrcoef = get_corrcoef(np.array(y.reshape(1, -1)),\n",
    "                        np.array(y_prime.reshape(1, -1)))\n",
    "print('Correlation coefficient: {}'.format(corrcoef))\n",
    "\n",
    "# 绘制岭轨迹\n",
    "ws = ridge_traj(X, y, ntest)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "lambdas = [i-10 for i in range(ntest)]\n",
    "ax.plot(lambdas, ws)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机梯度下降算法计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def RidgeGradientDescent(x, y, alpha, iters, L):\n",
    "    x=np.matrix(x)\n",
    "    y=np.matrix(y).transpose()\n",
    "    m, n = np.shape(x)\n",
    "    beta = np.matrix(np.ones(n)).transpose()\n",
    "    XT = x.transpose()\n",
    "    for i in range(0, iters):\n",
    "        y_hat = np.dot(x, beta)\n",
    "        residuals = y_hat - y\n",
    "        MSE = (residuals.transpose()*residuals)/len(x)\n",
    "        print \"iteration:\", i, \"MSE:\", MSE\n",
    "        ols_gradient = np.dot(XT, residuals) / m\n",
    "        beta = beta - alpha * (ols_gradient + (L/m)*beta)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "intercept = np.ones(len(X))\n",
    "X = np.append(intercept, X)\n",
    "X = np.reshape(X,(442,11))\n",
    "\n",
    "from scipy import stats\n",
    "Z = stats.zscore(X, axis=0)\n",
    "Y = stats.zscore(y)\n",
    "\n",
    "Mybetas = RidgeGradientDescent(Z,Y,.1,5000,.1)\n",
    "print(Mybetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> https://mubaris.com/2017/09/28/linear-regression-from-scratch/\n",
    "\n",
    "\n",
    "> http://blog.csdn.net/bryan__/article/details/52338151\n",
    "\n",
    "> https://ask.hellobi.com/blog/guodongwei1991/9076\n",
    "\n",
    "> http://web.mit.edu/zoya/www/linearRegression.pdf\n",
    "\n",
    "> http://www.cs.cmu.edu/~aarti/Class/10701_Spring14/slides/linear_regression.pdf\n",
    "\n",
    "> https://webdocs.cs.ualberta.ca/~rgreiner/C-466/SLIDES/3b-Regression.pdf\n",
    "\n",
    "> http://vda.univie.ac.at/Teaching/ML/15s/LectureNotes/03_regression.pdf\n",
    "\n",
    "> http://blog.csdn.net/qq_34531825/article/details/52689654\n",
    "\n",
    "> http://freemind.pluskid.org/machine-learning/sparsity-and-some-basics-of-l1-regularization/#ed61992b37932e208ae114be75e42a3e6dc34cb3 \n",
    "\n",
    "> http://www.52caml.com/head_first_ml/ml-chapter1-regression-family/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
